{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e58fa40-b8a7-4b7b-9495-0d68ab65bfb1",
   "metadata": {},
   "source": [
    "# 1-)Multiple Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f857c2-7c34-48b6-bbf1-7c86ef2fbfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import display \n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kurtosis, skew\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def Linear_Regression_Model(independent_variables,target_variable):\n",
    "    print(\"*********************************************Linear Regression Model***********************************************************\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "    lm=LinearRegression() \n",
    "    linear_model=lm.fit(X_train,y_train)\n",
    "    constant_of_model=linear_model.intercept_\n",
    "    coefficients_of_ind_var=linear_model.coef_ \n",
    "    \n",
    "    def coefficients_of_model(coefficients_of_ind_var,constant_of_model, X_train):\n",
    "        df1= pd.DataFrame(coefficients_of_ind_var, index=X_train.columns,columns=['Coefficient'])\n",
    "        df2 = pd.DataFrame(constant_of_model, index=[\"Constant\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    def Equation_of_Model(constant_of_model, coefficients_of_ind_var, X_train,y_train):\n",
    "        formula_list=[]\n",
    "        formula_list.append(str(\"%.3f\" % constant_of_model)) \n",
    "        for feature, coefficient in zip(X_train.columns.to_list(),coefficients_of_ind_var):\n",
    "            x=feature + \"*\"+ str(\"%.3f\" % coefficient)\n",
    "            formula_list.append(x)\n",
    "        formula=\" + \".join(formula_list)\n",
    "        print(\"Equation of Model: {} = {}\".format(y_train.to_frame().columns[0],formula))\n",
    "   \n",
    "    def optimum_root_mean_square_error(linear_model,X_train, X_test, y_train, y_test):\n",
    "        optimum_rmse_test=np.sqrt(-cross_val_score(linear_model, X_test, y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        optimum_rmse_train=np.sqrt(-cross_val_score(linear_model, X_train, y_train, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        df1=pd.DataFrame(optimum_rmse_test, index=[\"optimum_RMSE_test\"],columns=['Coefficient'])\n",
    "        df2=pd.DataFrame(optimum_rmse_train, index=[\"optimum_RMSE_train\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    def optimum_R_squred_Score(linear_model,X_train,y_train):\n",
    "        R2_score=cross_val_score(linear_model, X_train, y_train, cv = 10, scoring = \"r2\").mean()\n",
    "        df1=pd.DataFrame(R2_score, index=[\"optimum_R2_SCORE\"],columns=['Coefficient'])                  \n",
    "        return df1\n",
    "    \n",
    "    \n",
    "    def Distributions_and_Variance_of_Test_Residuals(linear_model,X_test,y_test,X_train,y_train):\n",
    "        predictions_test = linear_model.predict(X_test)#\n",
    "        residual_test= y_test-predictions_test\n",
    "        \n",
    "        predictions_train = linear_model.predict(X_train)\n",
    "        residual_train= y_train-predictions_train\n",
    "        \n",
    "        fig, axes = plt.subplots(1,4,figsize=(20,5))\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        \n",
    "        \n",
    "        df_test=pd.DataFrame({\"Actual_Dependent_Values\":y_test,\n",
    "                             \"Predicted_Dependent_Values\":predictions_test,\n",
    "                             \"Residuals\":residual_test}).reset_index(drop=True)\n",
    "        \n",
    "        df_train=pd.DataFrame({\"Actual_Dependent_Values\":y_train,\n",
    "                              \"Predicted_Dependent_Values\":predictions_train,\n",
    "                              \"Residuals\":residual_train}).reset_index(drop=True)\n",
    "        \n",
    "      \n",
    "        fig.suptitle(\"Distributions and Variance of Residuals \")\n",
    "\n",
    "        a= \"Skewness: %.2f\" % residual_test.skew()\n",
    "        b=\"Kurtosis: %.2f\" % residual_test.kurtosis()\n",
    "        c=\"Mean: %.2f\" %   residual_test.mean()\n",
    "        d=\"Median: %.2f\" % residual_test.median()\n",
    "\n",
    "        sns.distplot(df_test[\"Residuals\"], bins=20,ax=axes[0])\n",
    "        axes[0].set(title=\"{} {} {} {}\".format(a,b,c,d), \n",
    "                    xlabel=\"Value of Deviations\", \n",
    "                    ylabel=\"Frequency\") \n",
    "     \n",
    "    \n",
    "\n",
    "        sns.scatterplot(x=\"Actual_Dependent_Values\", y=\"Predicted_Dependent_Values\",data=df_test, ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                    xlabel=\" Actual Test Values\", \n",
    "                    ylabel=\"Predicted Test Values\")\n",
    "            \n",
    "\n",
    "\n",
    "        sns.lineplot(x=df_test.index,y=df_test[\"Residuals\"],data=df_test,ax=axes[2])\n",
    "        axes[2].set(title=\"Residuals of Test Values  \", \n",
    "                    xlabel=\"Indexes of Test Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "\n",
    "        sns.lineplot(x=df_train.index,y=df_train[\"Residuals\"],data=df_train,ax=axes[3])\n",
    "        axes[3].set(title=\"Residuals of Train Values  \", \n",
    "                    xlabel=\"Indexes of Train Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "    def Harmonies_of_Actual_values_with_Predicted_values(linear_model,y_train,X_train,y_test,X_test):\n",
    "        fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "        fig.suptitle(\"Distributions of Test & Train values and their harmonies with Predicted values\")\n",
    "        sns.distplot(y_train, hist=False ,color=\"r\", label=\"Actual Values\",ax=axes[0])\n",
    "        sns.distplot(linear_model.predict(X_train),hist=False,color=\"b\",label=\"Predicted Values\",ax=axes[0])\n",
    "        axes[0].set(title=\"Actual vs Predicted Train Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        sns.distplot(y_test, hist=False ,color=\"r\", label=\"Actual Value\",ax=axes[1])\n",
    "        sns.distplot(linear_model.predict(X_test),hist=False,color=\"b\",label=\"Predicted Value\",ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[1].legend(loc='best')\n",
    "\n",
    "    \n",
    "                \n",
    "                \n",
    "    a=coefficients_of_model(coefficients_of_ind_var,constant_of_model, X_train)\n",
    "    b=Equation_of_Model(constant_of_model, coefficients_of_ind_var, X_train,y_train)\n",
    "    c=optimum_root_mean_square_error(linear_model,X_train, X_test, y_train, y_test)\n",
    "    d= optimum_R_squred_Score(linear_model,X_train,y_train)\n",
    "    e= Harmonies_of_Actual_values_with_Predicted_values(linear_model,y_train,X_train,y_test,X_test)\n",
    "    f=  Distributions_and_Variance_of_Test_Residuals(linear_model,X_test,y_test,X_train,y_train)\n",
    "    return  display(a) ,b, display(c) ,display(d) , e , f\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c743c7-08ee-460c-a6a0-0ac868d8ee07",
   "metadata": {},
   "source": [
    "# 2-) Polynomial Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b53804-ef76-4c90-9df5-8efe54bd01c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import kurtosis, skew\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def Polynomial_Regression_Model(independent_variables, target_variable):\n",
    "    print(\"*************************************************Polynomial_Regression_Model*********************************************************\")\n",
    "    def Best_degree_of_polynomial_feature(independent_variables, target_variable):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "        rsqu_test=[]\n",
    "        order = [1,2,3,4,5,6,7,8,9,10]\n",
    "        for n in order:\n",
    "            polynomial_feature=PolynomialFeatures(degree=n)\n",
    "            X_train_pr = polynomial_feature.fit_transform(X_train)\n",
    "            X_test_pr = polynomial_feature.fit_transform(X_test)\n",
    "            scaled_X_train_pr=StandardScaler().fit_transform(X_train_pr)# \n",
    "            scaled_X_test_pr=StandardScaler().fit_transform(X_test_pr)\n",
    "            lm=LinearRegression()\n",
    "            polynomial_linear_model=lm.fit(scaled_X_train_pr,y_train)\n",
    "            R2_SCORE=cross_val_score(polynomial_linear_model, scaled_X_test_pr, y_test, cv = 10, scoring = \"r2\").mean()\n",
    "            rsqu_test.append(R2_SCORE)\n",
    "        best_degree_of_polynomial_feature = rsqu_test.index(max(rsqu_test))+1\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        plt.plot(order, rsqu_test)\n",
    "        plt.xlabel('Polynomial Feature Degree')\n",
    "        plt.ylabel('R^2 Score Value')\n",
    "        plt.ylim(0,1)\n",
    "        plt.title('R^2 Score vs Feature Degree Using Test Set ')\n",
    "        print(\"Highest R^2 Score Value is  {}  and its Polynomial Feature Degree is {}\".format(max(rsqu_test),best_degree_of_polynomial_feature))\n",
    "        return  best_degree_of_polynomial_feature\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "    polynomial_feature=PolynomialFeatures(degree=Best_degree_of_polynomial_feature(independent_variables, target_variable))\n",
    "    X_train_pr = polynomial_feature.fit_transform(X_train)\n",
    "    X_test_pr = polynomial_feature.fit_transform(X_test)\n",
    "    scaled_X_train_pr=StandardScaler().fit_transform(X_train_pr) \n",
    "    scaled_X_test_pr=StandardScaler().fit_transform(X_test_pr)\n",
    "    lm1=LinearRegression()\n",
    "    polynomial_linear_model_1=lm1.fit(scaled_X_train_pr,y_train)\n",
    "    constant_of_model=polynomial_linear_model_1.intercept_ \n",
    "    coefficients_of_ind_var=polynomial_linear_model_1.coef_\n",
    "    \n",
    "    def coefficients_of_model( coefficients_of_ind_var,constant_of_model,X_train):\n",
    "        df1= pd.DataFrame(coefficients_of_ind_var, index=polynomial_feature.get_feature_names(X_train.columns),columns=['Coefficient'])\n",
    "        df2 = pd.DataFrame(constant_of_model, index=[\"Constant\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    \n",
    "    def Equation_of_Model(constant_of_model, coefficients_of_ind_var, X_train,y_train):\n",
    "        formula_list=[]\n",
    "        formula_list.append(str(\"%.3f\" % constant_of_model))\n",
    "        for feature, coefficient in zip(polynomial_feature.get_feature_names(X_train.columns),coefficients_of_ind_var):\n",
    "            x=feature + \"*\"+ str(\"%.3f\" % coefficient)\n",
    "            formula_list.append(x)\n",
    "        formula=\" + \".join(formula_list)\n",
    "        print(\"Equation of Model: {} = {}\".format(y_train.to_frame().columns[0],formula))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def optimum_root_mean_square_error( polynomial_linear_model_1,scaled_X_train_pr, scaled_X_test_pr, y_train, y_test):\n",
    "        optimum_rmse_test=np.sqrt(-cross_val_score( polynomial_linear_model_1, scaled_X_test_pr, y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        optimum_rmse_train=np.sqrt(-cross_val_score( polynomial_linear_model_1, scaled_X_train_pr, y_train, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        df1=pd.DataFrame(optimum_rmse_test, index=[\"optimum_RMSE_test\"],columns=['Coefficient'])\n",
    "        df2=pd.DataFrame(optimum_rmse_train, index=[\"optimum_RMSE_train\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    \n",
    "    def optimum_R_squred_Score(polynomial_linear_model_1 ,scaled_X_train_pr, y_train):\n",
    "        R2_score=cross_val_score(polynomial_linear_model_1, scaled_X_train_pr , y_train, cv = 10, scoring = \"r2\").mean() \n",
    "        df1=pd.DataFrame(R2_score, index=[\"optimum_R2_SCORE\"],columns=['Coefficient'])                   \n",
    "        return df1\n",
    "    \n",
    "    \n",
    "    def Distributions_and_Variance_of_Test_Residuals(polynomial_linear_model_1,   scaled_X_test_pr,   y_test,   scaled_X_train_pr,   y_train):\n",
    "        predictions_test = polynomial_linear_model_1.predict(scaled_X_test_pr)\n",
    "        residual_test= y_test-predictions_test\n",
    "\n",
    "\n",
    "        predictions_train = polynomial_linear_model_1.predict(scaled_X_train_pr)\n",
    "        residual_train= y_train-predictions_train\n",
    "\n",
    "        fig, axes = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        df_test=pd.DataFrame({\"Actual_Dependent_Values\":y_test,\n",
    "                             \"Predicted_Dependent_Values\":predictions_test,\n",
    "                             \"Residuals\":residual_test}).reset_index(drop=True)\n",
    "        \n",
    "        df_train=pd.DataFrame({\"Actual_Dependent_Values\":y_train,\n",
    "                              \"Predicted_Dependent_Values\":predictions_train,\n",
    "                              \"Residuals\":residual_train}).reset_index(drop=True)\n",
    "              \n",
    "        fig.suptitle(\"Distributions and Variance of Residuals \")\n",
    "\n",
    "        a= \"Skewness: %.2f\" % residual_test.skew()\n",
    "        b=\"Kurtosis: %.2f\" % residual_test.kurtosis()\n",
    "        c=\"Mean: %.2f\" %   residual_test.mean()\n",
    "        d=\"Median: %.2f\" % residual_test.median()\n",
    "\n",
    "\n",
    "        sns.distplot(df_test[\"Residuals\"], bins=20,ax=axes[0])\n",
    "        axes[0].set(title=\"{} {} {} {}\".format(a,b,c,d), \n",
    "                    xlabel=\"Value of Deviations\", \n",
    "                    ylabel=\"Frequency\") \n",
    "     \n",
    "    \n",
    "\n",
    "        sns.scatterplot(x=\"Actual_Dependent_Values\", y=\"Predicted_Dependent_Values\",data=df_test, ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                    xlabel=\" Actual Test Values\", \n",
    "                    ylabel=\"Predicted Test Values\")\n",
    "            \n",
    "\n",
    "\n",
    "        sns.lineplot(x=df_test.index,y=df_test[\"Residuals\"],data=df_test,ax=axes[2])\n",
    "        axes[2].set(title=\"Residuals of Test Values  \", \n",
    "                    xlabel=\"Indexes of Test Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "\n",
    "        sns.lineplot(x=df_train.index,y=df_train[\"Residuals\"],data=df_train,ax=axes[3])\n",
    "        axes[3].set(title=\"Residuals of Train Values  \", \n",
    "                    xlabel=\"Indexes of Train Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "    \n",
    "    \n",
    "    def Harmonies_of_Actual_values_with_Predicted_values(polynomial_linear_model_1,   scaled_X_test_pr,   y_test,   scaled_X_train_pr,   y_train):\n",
    "        fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "        fig.suptitle(\"Distributions of Test & Train values and their harmonies with Predicted values\")\n",
    "        sns.distplot(y_train, hist=False ,color=\"r\", label=\"Actual Values\",ax=axes[0])\n",
    "        sns.distplot(polynomial_linear_model_1.predict(scaled_X_train_pr),hist=False,color=\"b\",label=\"Predicted Values\",ax=axes[0])\n",
    "        axes[0].set(title=\"Actual vs Predicted Train Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        sns.distplot(y_test, hist=False ,color=\"r\", label=\"Actual Value\",ax=axes[1])\n",
    "        sns.distplot(polynomial_linear_model_1.predict(scaled_X_test_pr),hist=False,color=\"b\",label=\"Predicted Value\",ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[1].legend(loc='best')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    a= Best_degree_of_polynomial_feature(independent_variables, target_variable)\n",
    "    b= coefficients_of_model( coefficients_of_ind_var,constant_of_model,X_train)\n",
    "    c=Equation_of_Model(constant_of_model, coefficients_of_ind_var, X_train,y_train)\n",
    "    d=optimum_root_mean_square_error( polynomial_linear_model_1,scaled_X_train_pr, scaled_X_test_pr, y_train, y_test)\n",
    "    e=optimum_R_squred_Score(polynomial_linear_model_1 ,scaled_X_train_pr, y_train)\n",
    "    f=Distributions_and_Variance_of_Test_Residuals(polynomial_linear_model_1,   scaled_X_test_pr,   y_test,   scaled_X_train_pr,   y_train)\n",
    "    g=Harmonies_of_Actual_values_with_Predicted_values(polynomial_linear_model_1,   scaled_X_test_pr,   y_test,   scaled_X_train_pr,   y_train)\n",
    "    \n",
    "    return display(b),c ,display(d), display(e),f ,g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0f1f49-1c32-4711-a900-f339dc569a6d",
   "metadata": {},
   "source": [
    "# 3-) Principal Component Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf752e6-e006-4f8f-800a-0bb935068f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import kurtosis, skew\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def Principal_Component_Regression_Model(independent_variables,target_variable):\n",
    "    print(\"************************************************Principal Component Regression*******************************************************\")\n",
    "    def Correlations_between_independent_variables(independent_variables):\n",
    "        plt.figure(figsize= (15, 15))\n",
    "        sns.heatmap(independent_variables.corr(),fmt=\".2g\",annot=True,linewidths=0.7)\n",
    "        \n",
    "    \n",
    "    def Best_Number_of_independent_variables(independent_variables,target_variable):\n",
    "        pca=PCA()\n",
    "        lm = LinearRegression()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(independent_variables,target_variable, test_size=0.25, random_state=101)\n",
    "        X_train_scaled=StandardScaler().fit_transform(X_train)\n",
    "        X_train_scaled_pca= pca.fit_transform(X_train_scaled)\n",
    "        cv_10 = KFold(n_splits = 10,shuffle = True, random_state = 1)\n",
    "        RMSE=[]\n",
    "        for i in np.arange(1, X_train_scaled_pca.shape[1] + 1):\n",
    "            score = np.sqrt(-cross_val_score(lm, \n",
    "                                             X_train_scaled_pca[:,:i], \n",
    "                                             y_train.ravel(), \n",
    "                                             cv=cv_10, \n",
    "                                             scoring='neg_mean_squared_error').mean())\n",
    "            RMSE.append(score)\n",
    "        best_number_of_independent_variables = RMSE.index(min(RMSE))+1\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        plt.plot(RMSE, '-v', color=\"red\")\n",
    "        plt.xlabel('Index of  Independent Variables')\n",
    "        plt.ylabel('Root Mean Square Error')\n",
    "        plt.xlim(-1,X_train_scaled_pca.shape[1] + 1)\n",
    "        plt.title('PCR Model Tuning for {}'.format(target_variable.to_frame().columns[0]))\n",
    "        print(\"Best number of independent variables is {}\".format(best_number_of_independent_variables))\n",
    "        return best_number_of_independent_variables\n",
    "    \n",
    "    b=Best_Number_of_independent_variables(independent_variables,target_variable)\n",
    "    pca=PCA()\n",
    "    lm = LinearRegression()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(independent_variables,target_variable, test_size=0.25, random_state=101)\n",
    "    X_train_scaled=StandardScaler().fit_transform(X_train)\n",
    "    X_train_scaled_pca= pca.fit_transform(X_train_scaled)\n",
    "    X_test_scaled=StandardScaler().fit_transform(X_test)\n",
    "    X_test_scaled_pca= pca.fit_transform(X_test_scaled)\n",
    "    pcr_model = lm.fit(X_train_scaled_pca[:,0:Best_Number_of_independent_variables(independent_variables,target_variable)], y_train)\n",
    "    constant_of_model=pcr_model .intercept_ \n",
    "    coefficients_of_ind_var=pcr_model.coef_\n",
    "    \n",
    "    def coefficients_of_model( coefficients_of_ind_var,constant_of_model,X_train):\n",
    "        df1= pd.DataFrame(coefficients_of_ind_var, index=X_train.columns[0:b],columns=['Coefficient'])\n",
    "        df2 = pd.DataFrame(constant_of_model, index=[\"Constant\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "   \n",
    "    def Equation_of_Model(constant_of_model, coefficients_of_ind_var, X_train,y_train):\n",
    "        formula_list=[]\n",
    "        formula_list.append(str(\"%.3f\" % constant_of_model))\n",
    "        for feature, coefficient in zip(X_train.columns[0:b],coefficients_of_ind_var):\n",
    "            x=feature + \"*\"+ str(\"%.3f\" % coefficient)\n",
    "            formula_list.append(x)\n",
    "        formula=\" + \".join(formula_list)\n",
    "        print(\"Equation of Model: {} = {}\".format(y_train.to_frame().columns[0],formula))\n",
    "    \n",
    "    \n",
    "    def optimum_root_mean_square_error( pcr_model, X_train_scaled_pca, X_test_scaled_pca, y_train, y_test):  \n",
    "        optimum_rmse_test=np.sqrt(-cross_val_score( pcr_model, X_test_scaled_pca[:,0:b], y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        optimum_rmse_train=np.sqrt(-cross_val_score( pcr_model, X_train_scaled_pca[:,0:b], y_train, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        df1=pd.DataFrame(optimum_rmse_test, index=[\"optimum_RMSE_test\"],columns=['Coefficient'])\n",
    "        df2=pd.DataFrame(optimum_rmse_train, index=[\"optimum_RMSE_train\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    def optimum_R_squred_Score(pcr_model ,X_train_scaled_pca, y_train, b):\n",
    "        R2_score=cross_val_score(pcr_model, X_train_scaled_pca[:,0:b] , y_train, cv = 10, scoring = \"r2\").mean()\n",
    "        df1=pd.DataFrame(R2_score, index=[\"optimum_R2_SCORE\"],columns=['Coefficient'])                   \n",
    "        return df1\n",
    "    \n",
    "    \n",
    "    def Distributions_and_Variance_of_Test_Residuals(pcr_model,   X_test_scaled_pca,   y_test,   X_train_scaled_pca,   y_train,b):\n",
    "        predictions_test = pcr_model.predict(X_test_scaled_pca[:,0:b])\n",
    "        residual_test= y_test-predictions_test\n",
    "\n",
    "\n",
    "        predictions_train = pcr_model.predict(X_train_scaled_pca[:,0:b])\n",
    "        residual_train= y_train-predictions_train\n",
    "\n",
    "        fig, axes = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        df_test=pd.DataFrame({\"Actual_Dependent_Values\":y_test,\n",
    "                             \"Predicted_Dependent_Values\":predictions_test,\n",
    "                             \"Residuals\":residual_test}).reset_index(drop=True)\n",
    "        \n",
    "        df_train=pd.DataFrame({\"Actual_Dependent_Values\":y_train,\n",
    "                              \"Predicted_Dependent_Values\":predictions_train,\n",
    "                              \"Residuals\":residual_train}).reset_index(drop=True)\n",
    "              \n",
    "        fig.suptitle(\"Distributions and Variance of Residuals \")\n",
    "\n",
    "        a= \"Skewness: %.2f\" % residual_test.skew()\n",
    "        b=\"Kurtosis: %.2f\" % residual_test.kurtosis()\n",
    "        c=\"Mean: %.2f\" %   residual_test.mean()\n",
    "        d=\"Median: %.2f\" % residual_test.median()\n",
    "\n",
    "\n",
    "        sns.distplot(df_test[\"Residuals\"], bins=20,ax=axes[0])\n",
    "        axes[0].set(title=\"{} {} {} {}\".format(a,b,c,d), \n",
    "                    xlabel=\"Value of Deviations\", \n",
    "                    ylabel=\"Frequency\") \n",
    "     \n",
    "    \n",
    "\n",
    "        sns.scatterplot(x=\"Actual_Dependent_Values\", y=\"Predicted_Dependent_Values\",data=df_test, ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                    xlabel=\" Actual Test Values\", \n",
    "                    ylabel=\"Predicted Test Values\")\n",
    "            \n",
    "\n",
    "\n",
    "        sns.lineplot(x=df_test.index,y=df_test[\"Residuals\"],data=df_test,ax=axes[2])\n",
    "        axes[2].set(title=\"Residuals of Test Values  \", \n",
    "                    xlabel=\"Indexes of Test Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "\n",
    "        sns.lineplot(x=df_train.index,y=df_train[\"Residuals\"],data=df_train,ax=axes[3])\n",
    "        axes[3].set(title=\"Residuals of Train Values  \", \n",
    "                    xlabel=\"Indexes of Train Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "        \n",
    "    def Harmonies_of_Actual_values_with_Predicted_values(pcr_model,   X_test_scaled_pca,   y_test,   X_train_scaled_pca,   y_train,b):\n",
    "        fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "        fig.suptitle(\"Distributions of Test & Train values and their harmonies with Predicted values\")\n",
    "        sns.distplot(y_train, hist=False ,color=\"r\", label=\"Actual Values\",ax=axes[0])\n",
    "        sns.distplot(pcr_model.predict(X_train_scaled_pca[:,0:b]),hist=False,color=\"b\",label=\"Predicted Values\",ax=axes[0])\n",
    "        axes[0].set(title=\"Actual vs Predicted Train Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        sns.distplot(y_test, hist=False ,color=\"r\", label=\"Actual Value\",ax=axes[1])\n",
    "        sns.distplot(pcr_model.predict(X_test_scaled_pca[:,0:b]),hist=False,color=\"b\",label=\"Predicted Value\",ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[1].legend(loc='best')\n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    a=Correlations_between_independent_variables(independent_variables)\n",
    "    b=Best_Number_of_independent_variables(independent_variables,target_variable)\n",
    "    c=coefficients_of_model( coefficients_of_ind_var,constant_of_model,X_train)\n",
    "    d=Equation_of_Model(constant_of_model, coefficients_of_ind_var, X_train,y_train)\n",
    "    f=optimum_root_mean_square_error( pcr_model, X_train_scaled_pca, X_test_scaled_pca, y_train, y_test)\n",
    "    g=optimum_R_squred_Score(pcr_model ,X_train_scaled_pca, y_train, b)\n",
    "    h=Distributions_and_Variance_of_Test_Residuals(pcr_model,   X_test_scaled_pca,   y_test,   X_train_scaled_pca,   y_train,b)\n",
    "    k=Harmonies_of_Actual_values_with_Predicted_values(pcr_model,   X_test_scaled_pca,   y_test,   X_train_scaled_pca,   y_train,b)\n",
    "    return b,display(c) ,d, display(f),display(g) ,h,k\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e088b840-ff21-47f4-8148-57b633d9aaa7",
   "metadata": {},
   "source": [
    "# 4-)Partial Least Squares Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc08021-ea30-4b64-b5ef-c78767677ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.cross_decomposition import PLSRegression, PLSSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import kurtosis, skew\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def Partial_Least_Squares_Regression_Model(independent_variables,target_variable):\n",
    "    print(\"************************************************Partial Least Squares Regression*******************************************************\")\n",
    "    \n",
    "    def Correlations_between_independent_variables(independent_variables):\n",
    "        plt.figure(figsize= (15, 15))\n",
    "        sns.heatmap(independent_variables.corr(),fmt=\".2g\",annot=True,linewidths=0.7)\n",
    "    \n",
    "    def Best_Number_of_independent_variables(independent_variables,target_variable):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(independent_variables,target_variable, test_size=0.25, random_state=101) \n",
    "        cv_10 = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "        RMSE = []\n",
    "        for i in np.arange(1, X_train.shape[1] + 1):\n",
    "            pls = PLSRegression(n_components=i)\n",
    "            score = np.sqrt(-cross_val_score(pls, X_train, y_train, cv=cv_10, scoring='neg_mean_squared_error').mean())\n",
    "            RMSE.append(score)\n",
    "\n",
    "        best_number_of_independent_variables = RMSE.index(min(RMSE))+1\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        plt.plot(RMSE, '-v', color=\"red\")\n",
    "        plt.xlabel('Index of  Independent Variables')\n",
    "        plt.ylabel('Root Mean Square Error')\n",
    "        plt.xlim(-1,X_train.shape[1] + 1)\n",
    "        plt.title('PLS Model Tuning for {}'.format(target_variable.to_frame().columns[0]))\n",
    "        plt.show\n",
    "        print(\"Best number of independent variables is {}\".format(best_number_of_independent_variables))\n",
    "        return best_number_of_independent_variables\n",
    "    \n",
    "    \n",
    "    b=Best_Number_of_independent_variables(independent_variables,target_variable)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(independent_variables,target_variable, test_size=0.25, random_state=101)\n",
    "    pls_model = PLSRegression(n_components = b).fit(X_train, y_train)\n",
    "    coefficients_of_ind_var=pls_model.coef_\n",
    "    \n",
    "    def coefficients_of_model( coefficients_of_ind_var,X_train):\n",
    "        df1= pd.DataFrame(coefficients_of_ind_var, index=X_train.columns,columns=['Coefficient'])\n",
    "        return df1\n",
    "   \n",
    "   \n",
    "    \n",
    "    def Equation_of_Model( coefficients_of_ind_var, X_train,y_train):\n",
    "        formula_list=[]\n",
    "        for feature, coefficient in zip(X_train.columns.to_list(),coefficients_of_ind_var):\n",
    "            x=feature + \"*\"+ str(\"%.3f\" % coefficient)\n",
    "            formula_list.append(x)\n",
    "        formula=\" + \".join(formula_list)\n",
    "        print(\"Equation of Model: {} = {}\".format(y_train.to_frame().columns[0],formula))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def optimum_root_mean_square_error( pls_model, X_train , X_test , y_train, y_test):\n",
    "        optimum_rmse_test=np.sqrt(-cross_val_score( pls_model, X_test , y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        optimum_rmse_train=np.sqrt(-cross_val_score( pls_model, X_train , y_train, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        df1=pd.DataFrame(optimum_rmse_test, index=[\"optimum_RMSE_test\"],columns=['Coefficient'])\n",
    "        df2=pd.DataFrame(optimum_rmse_train, index=[\"optimum_RMSE_train\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    def optimum_R_squred_Score(pls_model ,X_train , y_train ):\n",
    "        R2_score=cross_val_score(pls_model, X_train , y_train, cv = 10, scoring = \"r2\").mean()\n",
    "        df1=pd.DataFrame(R2_score, index=[\"optimum_R2_SCORE\"],columns=['Coefficient'])                   \n",
    "        return df1\n",
    "    \n",
    "    \n",
    "    def Distributions_and_Variance_of_Test_Residuals(pls_model, X_test, y_test,  X_train, y_train):\n",
    "        predictions_test = pls_model.predict(X_test)\n",
    "        predictions_test=np.reshape(predictions_test,len(predictions_test))\n",
    "        residual_test=y_test-predictions_test\n",
    "\n",
    "\n",
    "\n",
    "        predictions_train = pls_model.predict(X_train)\n",
    "        predictions_train=np.reshape(predictions_train,len(predictions_train))\n",
    "        residual_train=y_train-predictions_train\n",
    "\n",
    "        fig, axes = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        df_test=pd.DataFrame({\"Actual_Dependent_Values\":y_test,\n",
    "                             \"Predicted_Dependent_Values\":predictions_test,\n",
    "                             \"Residuals\":residual_test}).reset_index(drop=True)\n",
    "        \n",
    "        df_train=pd.DataFrame({\"Actual_Dependent_Values\":y_train,\n",
    "                              \"Predicted_Dependent_Values\":predictions_train,\n",
    "                              \"Residuals\":residual_train}).reset_index(drop=True)\n",
    "              \n",
    "        fig.suptitle(\"Distributions and Variance of Residuals \")\n",
    "\n",
    "        a= \"Skewness: %.2f\" % residual_test.skew()\n",
    "        b=\"Kurtosis: %.2f\" % residual_test.kurtosis()\n",
    "        c=\"Mean: %.2f\" %   residual_test.mean()\n",
    "        d=\"Median: %.2f\" % residual_test.median()\n",
    "\n",
    "\n",
    "        sns.distplot(df_test[\"Residuals\"], bins=20,ax=axes[0])\n",
    "        axes[0].set(title=\"{} {} {} {}\".format(a,b,c,d), \n",
    "                    xlabel=\"Value of Deviations\", \n",
    "                    ylabel=\"Frequency\") \n",
    "     \n",
    "    \n",
    "\n",
    "        sns.scatterplot(x=\"Actual_Dependent_Values\", y=\"Predicted_Dependent_Values\",data=df_test, ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                    xlabel=\" Actual Test Values\", \n",
    "                    ylabel=\"Predicted Test Values\")\n",
    "            \n",
    "\n",
    "\n",
    "        sns.lineplot(x=df_test.index,y=df_test[\"Residuals\"],data=df_test,ax=axes[2])\n",
    "        axes[2].set(title=\"Residuals of Test Values  \", \n",
    "                    xlabel=\"Indexes of Test Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "\n",
    "        sns.lineplot(x=df_train.index,y=df_train[\"Residuals\"],data=df_train,ax=axes[3])\n",
    "        axes[3].set(title=\"Residuals of Train Values  \", \n",
    "                    xlabel=\"Indexes of Train Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "    def Harmonies_of_Actual_values_with_Predicted_values(pls_model,   X_test,   y_test,   X_train,   y_train):\n",
    "            fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "            fig.suptitle(\"Distributions of Test & Train values and their harmonies with Predicted values\")\n",
    "            sns.distplot(y_train, hist=False ,color=\"r\", label=\"Actual Values\",ax=axes[0])\n",
    "            sns.distplot(pls_model.predict(X_train),hist=False,color=\"b\",label=\"Predicted Values\",ax=axes[0])\n",
    "            axes[0].set(title=\"Actual vs Predicted Train Values\", \n",
    "                    xlabel=y_train.to_frame().columns[0], \n",
    "                    ylabel=\"Proportion\")\n",
    "            axes[0].legend(loc='best')\n",
    "\n",
    "            sns.distplot(y_test, hist=False ,color=\"r\", label=\"Actual Value\",ax=axes[1])\n",
    "            sns.distplot(pls_model.predict(X_test),hist=False,color=\"b\",label=\"Predicted Value\",ax=axes[1])\n",
    "            axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                    xlabel=y_train.to_frame().columns[0], \n",
    "                    ylabel=\"Proportion\")\n",
    "            axes[1].legend(loc='best')\n",
    "        \n",
    "    \n",
    "    \n",
    "    a=Best_Number_of_independent_variables(independent_variables,target_variable)\n",
    "    b=Correlations_between_independent_variables(independent_variables)\n",
    "    c=coefficients_of_model( coefficients_of_ind_var,X_train)\n",
    "    d=Equation_of_Model( coefficients_of_ind_var, X_train,y_train) \n",
    "    e=optimum_root_mean_square_error( pls_model, X_train , X_test , y_train, y_test)\n",
    "    f=optimum_R_squred_Score(pls_model ,X_train , y_train)\n",
    "    g=Distributions_and_Variance_of_Test_Residuals(pls_model,   X_test,   y_test,   X_train, y_train)\n",
    "    h=Harmonies_of_Actual_values_with_Predicted_values(pls_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    \n",
    "    return a,b, display(c),d,display(e),display(f),g ,h\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca5ff75-c2d4-452d-97f4-1280acc8ef24",
   "metadata": {},
   "source": [
    "# 5-)Ridge Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82fd11-4c11-415f-bc43-ff36a87a842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import display \n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kurtosis, skew\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def Ridge_regression_Model(independent_variables, target_variable):\n",
    "    print(\"*********************************************Ridge Regression Model***********************************************************\")\n",
    "    \n",
    "    def best_alpha_value(independent_variables, target_variable):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "        lambdas = 10**np.linspace(10,-2,1000)*0.5 \n",
    "        ridge_cv = RidgeCV(alphas = lambdas, \n",
    "                   scoring = \"neg_mean_squared_error\",\n",
    "                   normalize = True)\n",
    "        ridge_cv.fit(X_train, y_train)\n",
    "        print(\"Best_alpha_value is {}\".format(ridge_cv.alpha_))\n",
    "        \n",
    "        return ridge_cv.alpha_\n",
    "    \n",
    "    a=best_alpha_value(independent_variables, target_variable)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "    ridge_model = Ridge(alpha = a,normalize = True).fit(X_train, y_train)\n",
    "    constant_of_model=ridge_model.intercept_\n",
    "    coefficients_of_ind_var=ridge_model.coef_ \n",
    "    \n",
    "    def coefficients_of_model( coefficients_of_ind_var,constant_of_model,X_train):\n",
    "        df1= pd.DataFrame(coefficients_of_ind_var, index=X_train.columns,columns=['Coefficient'])\n",
    "        df2 = pd.DataFrame(constant_of_model, index=[\"Constant\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    def Equation_of_Model(constant_of_model, coefficients_of_ind_var, X_train,y_train):\n",
    "        formula_list=[]\n",
    "        formula_list.append(str(\"%.3f\" % constant_of_model))\n",
    "        for feature, coefficient in zip(X_train.columns,coefficients_of_ind_var):\n",
    "            x=feature + \"*\"+ str(\"%.3f\" % coefficient)\n",
    "            formula_list.append(x)\n",
    "        formula=\" + \".join(formula_list)\n",
    "        print(\"Equation of Model: {} = {}\".format(y_train.to_frame().columns[0],formula))\n",
    "        \n",
    "        \n",
    "    def optimum_root_mean_square_error( ridge_model, X_train, X_test, y_train, y_test):\n",
    "        optimum_rmse_test=np.sqrt(-cross_val_score( ridge_model, X_test, y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        optimum_rmse_train=np.sqrt(-cross_val_score( ridge_model, X_train, y_train, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        df1=pd.DataFrame(optimum_rmse_test, index=[\"optimum_RMSE_test\"],columns=['Coefficient'])\n",
    "        df2=pd.DataFrame(optimum_rmse_train, index=[\"optimum_RMSE_train\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    def optimum_R_squred_Score(ridge_model ,X_train, y_train)\n",
    "        R2_score=cross_val_score(ridge_model, X_train , y_train, cv = 10, scoring = \"r2\").mean()\n",
    "        df1=pd.DataFrame(R2_score, index=[\"optimum_R2_SCORE\"],columns=['Coefficient'])                  \n",
    "        return df1\n",
    "    \n",
    "    \n",
    "    def Distributions_and_Variance_of_Test_Residuals(ridge_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        predictions_test = ridge_model.predict(X_test)\n",
    "        residual_test= y_test-predictions_test\n",
    "\n",
    "\n",
    "        predictions_train = ridge_model.predict(X_train)\n",
    "        residual_train= y_train-predictions_train\n",
    "\n",
    "        fig, axes = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        df_test=pd.DataFrame({\"Actual_Dependent_Values\":y_test,\n",
    "                             \"Predicted_Dependent_Values\":predictions_test,\n",
    "                             \"Residuals\":residual_test}).reset_index(drop=True)\n",
    "        \n",
    "        df_train=pd.DataFrame({\"Actual_Dependent_Values\":y_train,\n",
    "                              \"Predicted_Dependent_Values\":predictions_train,\n",
    "                              \"Residuals\":residual_train}).reset_index(drop=True)\n",
    "              \n",
    "        fig.suptitle(\"Distributions and Variance of Residuals \")\n",
    "\n",
    "        a= \"Skewness: %.2f\" % residual_test.skew()\n",
    "        b=\"Kurtosis: %.2f\" % residual_test.kurtosis()\n",
    "        c=\"Mean: %.2f\" %   residual_test.mean()\n",
    "        d=\"Median: %.2f\" % residual_test.median()\n",
    "\n",
    "\n",
    "        sns.distplot(df_test[\"Residuals\"], bins=20,ax=axes[0])\n",
    "        axes[0].set(title=\"{} {} {} {}\".format(a,b,c,d), \n",
    "                    xlabel=\"Value of Deviations\", \n",
    "                    ylabel=\"Frequency\") \n",
    "     \n",
    "    \n",
    "\n",
    "        sns.scatterplot(x=\"Actual_Dependent_Values\", y=\"Predicted_Dependent_Values\",data=df_test, ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                    xlabel=\" Actual Test Values\", \n",
    "                    ylabel=\"Predicted Test Values\")\n",
    "            \n",
    "\n",
    "\n",
    "        sns.lineplot(x=df_test.index,y=df_test[\"Residuals\"],data=df_test,ax=axes[2])\n",
    "        axes[2].set(title=\"Residuals of Test Values  \", \n",
    "                    xlabel=\"Indexes of Test Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "\n",
    "        sns.lineplot(x=df_train.index,y=df_train[\"Residuals\"],data=df_train,ax=axes[3])\n",
    "        axes[3].set(title=\"Residuals of Train Values  \", \n",
    "                    xlabel=\"Indexes of Train Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "    def Harmonies_of_Actual_values_with_Predicted_values(ridge_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "        fig.suptitle(\"Distributions of Test & Train values and their harmonies with Predicted values\")\n",
    "        sns.distplot(y_train, hist=False ,color=\"r\", label=\"Actual Values\",ax=axes[0])\n",
    "        sns.distplot(ridge_model.predict(X_train),hist=False,color=\"b\",label=\"Predicted Values\",ax=axes[0])\n",
    "        axes[0].set(title=\"Actual vs Predicted Train Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        sns.distplot(y_test, hist=False ,color=\"r\", label=\"Actual Value\",ax=axes[1])\n",
    "        sns.distplot(ridge_model.predict(X_test),hist=False,color=\"b\",label=\"Predicted Value\",ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[1].legend(loc='best')\n",
    "    \n",
    "    b=coefficients_of_model( coefficients_of_ind_var,constant_of_model,X_train) \n",
    "    c=Equation_of_Model(constant_of_model, coefficients_of_ind_var, X_train,y_train)\n",
    "    d=optimum_root_mean_square_error( ridge_model, X_train, X_test, y_train, y_test)\n",
    "    e=optimum_R_squred_Score(ridge_model ,X_train, y_train)\n",
    "    f=Distributions_and_Variance_of_Test_Residuals(ridge_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    g=Harmonies_of_Actual_values_with_Predicted_values(ridge_model,   X_test,   y_test,   X_train,   y_train)\n",
    "        \n",
    "    return a,display(b),c,display(d),display(e),f,g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d50a59-3e28-4b82-a5a5-bc9be45b2286",
   "metadata": {},
   "source": [
    "# 6-)Lasso Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8848787-2a5a-4b49-b7e6-6aab174caf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import display # dataframe yapisindaki sonuclari cikti olarak gostermek icin kullaniriz\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kurtosis, skew\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def Lasso_Regression_Model(independent_variables, target_variable):\n",
    "    print(\"*********************************************Lasso Regression Model***********************************************************\")\n",
    "    \n",
    "    def best_alpha_value(independent_variables, target_variable):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "        lasso_cv_model = LassoCV(alphas = None, \n",
    "                         cv = 10, \n",
    "                         max_iter = 10000, \n",
    "                         normalize = True)\n",
    "        lasso_cv_model.fit(X_train,y_train)\n",
    "        print(\"Best_alpha_value is {}\".format(lasso_cv_model.alpha_))\n",
    "        \n",
    "        return lasso_cv_model.alpha_\n",
    "    \n",
    "    a=best_alpha_value(independent_variables, target_variable)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "    lasso_model = Lasso(alpha = a,normalize = True).fit(X_train, y_train)\n",
    "    constant_of_model=lasso_model.intercept_\n",
    "    coefficients_of_ind_var=lasso_model.coef_ \n",
    "    \n",
    "    def coefficients_of_model( coefficients_of_ind_var,constant_of_model,X_train):\n",
    "        df1= pd.DataFrame(coefficients_of_ind_var, index=X_train.columns,columns=['Coefficient'])\n",
    "        df2 = pd.DataFrame(constant_of_model, index=[\"Constant\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    def Equation_of_Model(constant_of_model, coefficients_of_ind_var, X_train,y_train):\n",
    "        formula_list=[]\n",
    "        formula_list.append(str(\"%.3f\" % constant_of_model))\n",
    "        for feature, coefficient in zip(X_train.columns,coefficients_of_ind_var):\n",
    "            if coefficient== 0:\n",
    "                continue\n",
    "            x=feature + \"*\"+ str(\"%.3f\" % coefficient)\n",
    "            formula_list.append(x)\n",
    "        formula=\" + \".join(formula_list)\n",
    "        print(\"Equation of Model: {} = {}\".format(y_train.to_frame().columns[0],formula))\n",
    "        \n",
    "        \n",
    "    def optimum_root_mean_square_error( lasso_model, X_train, X_test, y_train, y_test):\n",
    "        optimum_rmse_test=np.sqrt(-cross_val_score( lasso_model, X_test, y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        optimum_rmse_train=np.sqrt(-cross_val_score( lasso_model, X_train, y_train, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        df1=pd.DataFrame(optimum_rmse_test, index=[\"optimum_RMSE_test\"],columns=['Coefficient'])\n",
    "        df2=pd.DataFrame(optimum_rmse_train, index=[\"optimum_RMSE_train\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    def optimum_R_squred_Score(lasso_model ,X_train, y_train):\n",
    "        R2_score=cross_val_score(lasso_model, X_train , y_train, cv = 10, scoring = \"r2\").mean()\n",
    "        df1=pd.DataFrame(R2_score, index=[\"optimum_R2_SCORE\"],columns=['Coefficient'])                   \n",
    "        return df1\n",
    "    \n",
    "    \n",
    "    def Distributions_and_Variance_of_Test_Residuals(lasso_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        predictions_test = lasso_model.predict(X_test)\n",
    "        residual_test= y_test-predictions_test\n",
    "\n",
    "\n",
    "        predictions_train = lasso_model.predict(X_train)\n",
    "        residual_train= y_train-predictions_train\n",
    "\n",
    "        fig, axes = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        df_test=pd.DataFrame({\"Actual_Dependent_Values\":y_test,\n",
    "                             \"Predicted_Dependent_Values\":predictions_test,\n",
    "                             \"Residuals\":residual_test}).reset_index(drop=True)\n",
    "        \n",
    "        df_train=pd.DataFrame({\"Actual_Dependent_Values\":y_train,\n",
    "                              \"Predicted_Dependent_Values\":predictions_train,\n",
    "                              \"Residuals\":residual_train}).reset_index(drop=True)\n",
    "              \n",
    "        fig.suptitle(\"Distributions and Variance of Residuals \")\n",
    "\n",
    "        a= \"Skewness: %.2f\" % residual_test.skew()\n",
    "        b=\"Kurtosis: %.2f\" % residual_test.kurtosis()\n",
    "        c=\"Mean: %.2f\" %   residual_test.mean()\n",
    "        d=\"Median: %.2f\" % residual_test.median()\n",
    "\n",
    "\n",
    "        sns.distplot(df_test[\"Residuals\"], bins=20,ax=axes[0])\n",
    "        axes[0].set(title=\"{} {} {} {}\".format(a,b,c,d), \n",
    "                    xlabel=\"Value of Deviations\", \n",
    "                    ylabel=\"Frequency\") \n",
    "     \n",
    "    \n",
    "\n",
    "        sns.scatterplot(x=\"Actual_Dependent_Values\", y=\"Predicted_Dependent_Values\",data=df_test, ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                    xlabel=\" Actual Test Values\", \n",
    "                    ylabel=\"Predicted Test Values\")\n",
    "            \n",
    "\n",
    "\n",
    "        sns.lineplot(x=df_test.index,y=df_test[\"Residuals\"],data=df_test,ax=axes[2])\n",
    "        axes[2].set(title=\"Residuals of Test Values  \", \n",
    "                    xlabel=\"Indexes of Test Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "\n",
    "        sns.lineplot(x=df_train.index,y=df_train[\"Residuals\"],data=df_train,ax=axes[3])\n",
    "        axes[3].set(title=\"Residuals of Train Values  \", \n",
    "                    xlabel=\"Indexes of Train Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "        \n",
    "    def Harmonies_of_Actual_values_with_Predicted_values(lasso_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "        fig.suptitle(\"Distributions of Test & Train values and their harmonies with Predicted values\")\n",
    "        sns.distplot(y_train, hist=False ,color=\"r\", label=\"Actual Values\",ax=axes[0])\n",
    "        sns.distplot(lasso_model.predict(X_train),hist=False,color=\"b\",label=\"Predicted Values\",ax=axes[0])\n",
    "        axes[0].set(title=\"Actual vs Predicted Train Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        sns.distplot(y_test, hist=False ,color=\"r\", label=\"Actual Value\",ax=axes[1])\n",
    "        sns.distplot(lasso_model.predict(X_test),hist=False,color=\"b\",label=\"Predicted Value\",ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[1].legend(loc='best')\n",
    "    \n",
    "    b=coefficients_of_model( coefficients_of_ind_var,constant_of_model,X_train)\n",
    "    c=Equation_of_Model(constant_of_model, coefficients_of_ind_var, X_train,y_train)\n",
    "    d=optimum_root_mean_square_error( lasso_model, X_train, X_test, y_train, y_test)\n",
    "    e=optimum_R_squred_Score(lasso_model ,X_train, y_train)\n",
    "    f=Distributions_and_Variance_of_Test_Residuals(lasso_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    g=Harmonies_of_Actual_values_with_Predicted_values(lasso_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    \n",
    "    \n",
    "    return a ,display(b),c ,display(d),display(e), f ,g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae730ba1-ac47-44b2-83e3-a4f7d228e93a",
   "metadata": {},
   "source": [
    "# 7-)Elastic Net Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946d4b7-abe6-466b-bdbb-ec82c0b0ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import display \n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kurtosis, skew\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def Elastic_Net_Regression_Model(independent_variables, target_variable):\n",
    "    print(\"*********************************************Elastic Net Regression Model***********************************************************\")\n",
    "    \n",
    "    def best_alpha_value(independent_variables, target_variable):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "        elastic_cv_model = ElasticNetCV(cv = 10,\n",
    "                                        normalize=True,\n",
    "                                        max_iter = 10000).fit(X_train, y_train)\n",
    "        print(\"Best_alpha_value is {}\".format(elastic_cv_model.alpha_))\n",
    "        return elastic_cv_model.alpha_\n",
    "    \n",
    "    a=best_alpha_value(independent_variables, target_variable)\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "    elastic_net_model = ElasticNet(alpha = a,normalize = True).fit(X_train, y_train)\n",
    "    constant_of_model=elastic_net_model.intercept_\n",
    "    coefficients_of_ind_var=elastic_net_model.coef_ \n",
    "    \n",
    "    def coefficients_of_model( coefficients_of_ind_var,constant_of_model,X_train):\n",
    "        df1= pd.DataFrame(coefficients_of_ind_var, index=X_train.columns,columns=['Coefficient'])\n",
    "        df2 = pd.DataFrame(constant_of_model, index=[\"Constant\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    \n",
    "    def Equation_of_Model(constant_of_model, coefficients_of_ind_var, X_train,y_train):\n",
    "        formula_list=[]\n",
    "        formula_list.append(str(\"%.3f\" % constant_of_model))\n",
    "        for feature, coefficient in zip(X_train.columns,coefficients_of_ind_var):\n",
    "            x=feature + \"*\"+ str(\"%.3f\" % coefficient)\n",
    "            formula_list.append(x)\n",
    "        formula=\" + \".join(formula_list)\n",
    "        print(\"Equation of Model: {} = {}\".format(y_train.to_frame().columns[0],formula))\n",
    "    \n",
    "    def optimum_root_mean_square_error( elastic_net_model, X_train, X_test, y_train, y_test):\n",
    "        optimum_rmse_test=np.sqrt(-cross_val_score( elastic_net_model, X_test, y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        optimum_rmse_train=np.sqrt(-cross_val_score( elastic_net_model, X_train, y_train, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        df1=pd.DataFrame(optimum_rmse_test, index=[\"optimum_RMSE_test\"],columns=['Coefficient'])\n",
    "        df2=pd.DataFrame(optimum_rmse_train, index=[\"optimum_RMSE_train\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    def optimum_R_squred_Score(elastic_net_model ,X_train, y_train):\n",
    "        R2_score=cross_val_score(elastic_net_model, X_train , y_train, cv = 10, scoring = \"r2\").mean()\n",
    "        df1=pd.DataFrame(R2_score, index=[\"optimum_R2_SCORE\"],columns=['Coefficient'])                   \n",
    "        return df1\n",
    "    \n",
    "    def Distributions_and_Variance_of_Test_Residuals(elastic_net_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        predictions_test = elastic_net_model.predict(X_test)\n",
    "        residual_test= y_test-predictions_test\n",
    "\n",
    "\n",
    "        predictions_train = elastic_net_model.predict(X_train)\n",
    "        residual_train= y_train-predictions_train\n",
    "\n",
    "        fig, axes = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        df_test=pd.DataFrame({\"Actual_Dependent_Values\":y_test,\n",
    "                             \"Predicted_Dependent_Values\":predictions_test,\n",
    "                             \"Residuals\":residual_test}).reset_index(drop=True)\n",
    "        \n",
    "        df_train=pd.DataFrame({\"Actual_Dependent_Values\":y_train,\n",
    "                              \"Predicted_Dependent_Values\":predictions_train,\n",
    "                              \"Residuals\":residual_train}).reset_index(drop=True)\n",
    "              \n",
    "        fig.suptitle(\"Distributions and Variance of Residuals \")\n",
    "\n",
    "        a= \"Skewness: %.2f\" % residual_test.skew()\n",
    "        b=\"Kurtosis: %.2f\" % residual_test.kurtosis()\n",
    "        c=\"Mean: %.2f\" %   residual_test.mean()\n",
    "        d=\"Median: %.2f\" % residual_test.median()\n",
    "\n",
    "\n",
    "        sns.distplot(df_test[\"Residuals\"], bins=20,ax=axes[0])\n",
    "        axes[0].set(title=\"{} {} {} {}\".format(a,b,c,d), \n",
    "                    xlabel=\"Value of Deviations\", \n",
    "                    ylabel=\"Frequency\") \n",
    "     \n",
    "    \n",
    "\n",
    "        sns.scatterplot(x=\"Actual_Dependent_Values\", y=\"Predicted_Dependent_Values\",data=df_test, ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                    xlabel=\" Actual Test Values\", \n",
    "                    ylabel=\"Predicted Test Values\")\n",
    "            \n",
    "\n",
    "\n",
    "        sns.lineplot(x=df_test.index,y=df_test[\"Residuals\"],data=df_test,ax=axes[2])\n",
    "        axes[2].set(title=\"Residuals of Test Values  \", \n",
    "                    xlabel=\"Indexes of Test Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "\n",
    "        sns.lineplot(x=df_train.index,y=df_train[\"Residuals\"],data=df_train,ax=axes[3])\n",
    "        axes[3].set(title=\"Residuals of Train Values  \", \n",
    "                    xlabel=\"Indexes of Train Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "        \n",
    "    def Harmonies_of_Actual_values_with_Predicted_values(elastic_net_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "        fig.suptitle(\"Distributions of Test & Train values and their harmonies with Predicted values\")\n",
    "        sns.distplot(y_train, hist=False ,color=\"r\", label=\"Actual Values\",ax=axes[0])\n",
    "        sns.distplot(elastic_net_model.predict(X_train),hist=False,color=\"b\",label=\"Predicted Values\",ax=axes[0])\n",
    "        axes[0].set(title=\"Actual vs Predicted Train Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        sns.distplot(y_test, hist=False ,color=\"r\", label=\"Actual Value\",ax=axes[1])\n",
    "        sns.distplot(elastic_net_model.predict(X_test),hist=False,color=\"b\",label=\"Predicted Value\",ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[1].legend(loc='best')    \n",
    "    \n",
    "    b=coefficients_of_model( coefficients_of_ind_var,constant_of_model,X_train)\n",
    "    c=Equation_of_Model(constant_of_model, coefficients_of_ind_var, X_train,y_train)\n",
    "    d=optimum_root_mean_square_error( elastic_net_model, X_train, X_test, y_train, y_test)\n",
    "    e=optimum_R_squred_Score(elastic_net_model ,X_train, y_train)\n",
    "    f=Distributions_and_Variance_of_Test_Residuals(elastic_net_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    g=Harmonies_of_Actual_values_with_Predicted_values(elastic_net_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    \n",
    "    return  a, display(b) ,c ,display(d),display(e),f,g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6908aec-c17c-4fce-ae53-f5a40a5a97a4",
   "metadata": {},
   "source": [
    "# Codes are running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91a8f58-4be1-4971-8e92-69cdbf80d903",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Linear_Regression_Model(independent_variables,target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ddeeb2-701e-44ef-83ee-9c3087e0c0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Polynomial_Regression_Model(independent_variables, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a950a97-d7a8-44d0-9adf-0dd778c21cb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Principal_Component_Regression_Model(independent_variables,target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d557149-1135-49ca-9576-3a43eedad10c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Partial_Least_Squares_Regression_Model(independent_variables,target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83236c-4bee-46a0-9c82-5287af5b80e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Ridge_regression_Model(independent_variables, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a873ac76-d0ed-4631-ba7a-5cf6dffb07c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Lasso_Regression_Model(independent_variables, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f13db-9ef7-424e-a301-315ab17e5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Elastic_Net_Regression_Model(independent_variables, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14224c0b-c703-4985-bb08-fcf83ffc7fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee198db-121c-4ff4-986a-1b45b73468b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b3430-0bcf-4ec1-8352-57d7c8af7e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811b32aa-eb5f-40bd-8bed-77f047fe6271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
