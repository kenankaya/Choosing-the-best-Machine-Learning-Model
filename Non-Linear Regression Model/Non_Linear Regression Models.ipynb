{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab2e16a4-ebc8-44bf-8a32-c4c0ba099032",
   "metadata": {},
   "source": [
    "# 1-)K-Nearest Neigbors  Regression Model (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5491ab-2280-48fd-bc45-2aae560a6788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from scipy.stats import kurtosis, skew\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "\n",
    "\n",
    "def K_Nearest_Neighbors_Regression_Model(independent_variables, target_variable):\n",
    "    print(\"*************************************************K Nearest Neighbors Regression Model*********************************************************\")\n",
    "    \n",
    "    def optimum_number_of_neighbors(independent_variables, target_variable):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "        knn_params = {'n_neighbors': np.arange(1,30,1)}\n",
    "        knn = KNeighborsRegressor()\n",
    "        knn_cv_model = GridSearchCV(knn, knn_params, cv = 10)\n",
    "        knn_cv_model.fit(X_train, y_train)\n",
    "        optimum_number_of_neighbors=knn_cv_model.best_params_[\"n_neighbors\"]\n",
    "        print(\"Optimum number of neighbors is {}\".format(optimum_number_of_neighbors))\n",
    "        \n",
    "        return optimum_number_of_neighbors\n",
    "              \n",
    "    a=optimum_number_of_neighbors(independent_variables, target_variable) \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "    knn_model=KNeighborsRegressor(n_neighbors = a).fit(X_train, y_train)\n",
    "    \n",
    "    def optimum_root_mean_square_error( knn_model, X_train, X_test, y_train, y_test):# Bu kismda  diger  modelerle karsilastirip en iyi modeli secmek icin  opitimum rmse kullanacagiz  \n",
    "        optimum_rmse_test=np.sqrt(-cross_val_score( knn_model, X_test, y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean()#K KATLI cross validtion  uygulama yaptiktan sonnraki TEST hatamiz\n",
    "        optimum_rmse_train=np.sqrt(-cross_val_score( knn_model, X_train, y_train, cv = 10, scoring = \"neg_mean_squared_error\")).mean()#K KATLI cross validtion  uygulama yaptiktan sonnraki TRAIN hatamiz\n",
    "        df1=pd.DataFrame(optimum_rmse_test, index=[\"optimum_RMSE_test\"],columns=['Coefficient'])\n",
    "        df2=pd.DataFrame(optimum_rmse_train, index=[\"optimum_RMSE_train\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    \n",
    "    def optimum_R_squred_Score(knn_model ,X_train, y_train):#R^2 (R-square degeridir).#Yani bagimsiz degiskenler  bagimli degiskendeki varyasin yuzde kacini aciklamislardir\n",
    "        R2_score=cross_val_score(knn_model, X_train , y_train, cv = 10, scoring = \"r2\").mean()#burada egitim setimiz icerisinden her saferinde farkli fozlemler alindigindan dolayi \n",
    "        df1=pd.DataFrame(R2_score, index=[\"optimum_R2_SCORE\"],columns=['Coefficient'])                   #10 adet farkli  r2 score  degeri olustur duk ve bunlarin ortalam degerlerini aldik\n",
    "        return df1\n",
    "    \n",
    "    \n",
    "    \n",
    "    def Distributions_and_Variance_of_Test_Residuals(knn_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        predictions_test = knn_model.predict(X_test)# Olusturdugumuz modelin test degerleri icin olusturdugu tahmini degerler\n",
    "        residual_test= y_test-predictions_test\n",
    "\n",
    "\n",
    "        predictions_train = knn_model.predict(X_train)# Olusturdugumuz modelin train degerleri icin olusturdugu tahmini degerler\n",
    "        residual_train= y_train-predictions_train\n",
    "\n",
    "        fig, axes = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        df_test=pd.DataFrame({\"Actual_Dependent_Values\":y_test,\n",
    "                             \"Predicted_Dependent_Values\":predictions_test,\n",
    "                             \"Residuals\":residual_test}).reset_index(drop=True)\n",
    "        \n",
    "        df_train=pd.DataFrame({\"Actual_Dependent_Values\":y_train,\n",
    "                              \"Predicted_Dependent_Values\":predictions_train,\n",
    "                              \"Residuals\":residual_train}).reset_index(drop=True)\n",
    "              \n",
    "        fig.suptitle(\"Distributions and Variance of Residuals \")\n",
    "\n",
    "        a= \"Skewness: %.2f\" % residual_test.skew()\n",
    "        b=\"Kurtosis: %.2f\" % residual_test.kurtosis()\n",
    "        c=\"Mean: %.2f\" %   residual_test.mean()\n",
    "        d=\"Median: %.2f\" % residual_test.median()\n",
    "\n",
    "\n",
    "        sns.distplot(df_test[\"Residuals\"], bins=20,ax=axes[0])\n",
    "        axes[0].set(title=\"{} {} {} {}\".format(a,b,c,d), \n",
    "                    xlabel=\"Value of Deviations\", \n",
    "                    ylabel=\"Frequency\") \n",
    "     \n",
    "    \n",
    "\n",
    "        sns.scatterplot(x=\"Actual_Dependent_Values\", y=\"Predicted_Dependent_Values\",data=df_test, ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                    xlabel=\" Actual Test Values\", \n",
    "                    ylabel=\"Predicted Test Values\")\n",
    "            \n",
    "\n",
    "\n",
    "        sns.lineplot(x=df_test.index,y=df_test[\"Residuals\"],data=df_test,ax=axes[2])\n",
    "        axes[2].set(title=\"Residuals of Test Values  \", \n",
    "                    xlabel=\"Indexes of Test Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "\n",
    "        sns.lineplot(x=df_train.index,y=df_train[\"Residuals\"],data=df_train,ax=axes[3])\n",
    "        axes[3].set(title=\"Residuals of Train Values  \", \n",
    "                    xlabel=\"Indexes of Train Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "        \n",
    "    def Harmonies_of_Actual_values_with_Predicted_values(knn_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "        fig.suptitle(\"Distributions of Test & Train values and their harmonies with Predicted values\")\n",
    "        sns.distplot(y_train, hist=False ,color=\"r\", label=\"Actual Values\",ax=axes[0])\n",
    "        sns.distplot(knn_model.predict(X_train),hist=False,color=\"b\",label=\"Predicted Values\",ax=axes[0])\n",
    "        axes[0].set(title=\"Actual vs Predicted Train Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        sns.distplot(y_test, hist=False ,color=\"r\", label=\"Actual Value\",ax=axes[1])\n",
    "        sns.distplot(knn_model.predict(X_test),hist=False,color=\"b\",label=\"Predicted Value\",ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[1].legend(loc='best')    \n",
    "    \n",
    "    b=optimum_root_mean_square_error( knn_model, X_train, X_test, y_train, y_test)\n",
    "    c=optimum_R_squred_Score(knn_model ,X_train, y_train)\n",
    "    d=Distributions_and_Variance_of_Test_Residuals(knn_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    e=Harmonies_of_Actual_values_with_Predicted_values(knn_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    \n",
    "    return display(b),display(c),d ,e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d643a2-0cea-4d5c-bee9-216d04827614",
   "metadata": {},
   "source": [
    "# 2-)Support Vector Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ff4dd-a48b-4ca4-bd15-4d5a64ede935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.svm import SVR\n",
    "from scipy.stats import kurtosis, skew\n",
    "from IPython.display import display\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "\n",
    "def Support_Vector_Regression_Model(independent_variables, target_variable):\n",
    "    print(\"************************************************* Support Vector Regression Model*********************************************************\")\n",
    "    \n",
    "    def optimum_value_of_complexity_parameter(independent_variables, target_variable):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "        svr_params = {\"C\": np.linspace(0.1,10000,600)}\n",
    "        svr_model=SVR()\n",
    "        svr_cv_model = GridSearchCV(svr_model,svr_params, cv = 10,n_jobs=-1).fit(X_train, y_train)\n",
    "        optimum_parameter=svr_cv_model.best_params_[\"C\"]\n",
    "        print(\"Optimum value of complexity parameter is {} \".format(optimum_parameter))\n",
    "        return optimum_parameter\n",
    "    a=optimum_value_of_complexity_parameter(independent_variables, target_variable)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "    svr_model=SVR(C=a).fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    def optimum_root_mean_square_error( svr_model, X_train, X_test, y_train, y_test):# Bu kismda  diger  modelerle karsilastirip en iyi modeli secmek icin  opitimum rmse kullanacagiz  \n",
    "        optimum_rmse_test=np.sqrt(-cross_val_score( svr_model, X_test, y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean()#K KATLI cross validtion  uygulama yaptiktan sonnraki TEST hatamiz\n",
    "        optimum_rmse_train=np.sqrt(-cross_val_score( svr_model, X_train, y_train, cv = 10, scoring = \"neg_mean_squared_error\")).mean()#K KATLI cross validtion  uygulama yaptiktan sonnraki TRAIN hatamiz\n",
    "        df1=pd.DataFrame(optimum_rmse_test, index=[\"optimum_RMSE_test\"],columns=['Coefficient'])\n",
    "        df2=pd.DataFrame(optimum_rmse_train, index=[\"optimum_RMSE_train\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    def optimum_R_squred_Score(svr_model ,X_train, y_train):#R^2 (R-square degeridir).#Yani bagimsiz degiskenler  bagimli degiskendeki varyasin yuzde kacini aciklamislardir\n",
    "        R2_score=cross_val_score(svr_model, X_train , y_train, cv = 10, scoring = \"r2\").mean()#burada egitim setimiz icerisinden her saferinde farkli fozlemler alindigindan dolayi \n",
    "        df1=pd.DataFrame(R2_score, index=[\"optimum_R2_SCORE\"],columns=['Coefficient'])                   #10 adet farkli  r2 score  degeri olustur duk ve bunlarin ortalam degerlerini aldik\n",
    "        return df1\n",
    "    \n",
    "    def Distributions_and_Variance_of_Test_Residuals(svr_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        predictions_test = svr_model.predict(X_test)# Olusturdugumuz modelin test degerleri icin olusturdugu tahmini degerler\n",
    "        residual_test= y_test-predictions_test\n",
    "\n",
    "\n",
    "        predictions_train = svr_model.predict(X_train)# Olusturdugumuz modelin train degerleri icin olusturdugu tahmini degerler\n",
    "        residual_train= y_train-predictions_train\n",
    "\n",
    "        fig, axes = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        df_test=pd.DataFrame({\"Actual_Dependent_Values\":y_test,\n",
    "                             \"Predicted_Dependent_Values\":predictions_test,\n",
    "                             \"Residuals\":residual_test}).reset_index(drop=True)\n",
    "        \n",
    "        df_train=pd.DataFrame({\"Actual_Dependent_Values\":y_train,\n",
    "                              \"Predicted_Dependent_Values\":predictions_train,\n",
    "                              \"Residuals\":residual_train}).reset_index(drop=True)\n",
    "              \n",
    "        fig.suptitle(\"Distributions and Variance of Residuals \")\n",
    "\n",
    "        a= \"Skewness: %.2f\" % residual_test.skew()\n",
    "        b=\"Kurtosis: %.2f\" % residual_test.kurtosis()\n",
    "        c=\"Mean: %.2f\" %   residual_test.mean()\n",
    "        d=\"Median: %.2f\" % residual_test.median()\n",
    "\n",
    "\n",
    "        sns.distplot(df_test[\"Residuals\"], bins=20,ax=axes[0])\n",
    "        axes[0].set(title=\"{} {} {} {}\".format(a,b,c,d), \n",
    "                    xlabel=\"Value of Deviations\", \n",
    "                    ylabel=\"Frequency\") \n",
    "     \n",
    "    \n",
    "\n",
    "        sns.scatterplot(x=\"Actual_Dependent_Values\", y=\"Predicted_Dependent_Values\",data=df_test, ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                    xlabel=\" Actual Test Values\", \n",
    "                    ylabel=\"Predicted Test Values\")\n",
    "            \n",
    "\n",
    "\n",
    "        sns.lineplot(x=df_test.index,y=df_test[\"Residuals\"],data=df_test,ax=axes[2])\n",
    "        axes[2].set(title=\"Residuals of Test Values  \", \n",
    "                    xlabel=\"Indexes of Test Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "\n",
    "        sns.lineplot(x=df_train.index,y=df_train[\"Residuals\"],data=df_train,ax=axes[3])\n",
    "        axes[3].set(title=\"Residuals of Train Values  \", \n",
    "                    xlabel=\"Indexes of Train Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "        \n",
    "    def Harmonies_of_Actual_values_with_Predicted_values(svr_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "        fig.suptitle(\"Distributions of Test & Train values and their harmonies with Predicted values\")\n",
    "        sns.distplot(y_train, hist=False ,color=\"r\", label=\"Actual Values\",ax=axes[0])\n",
    "        sns.distplot(svr_model.predict(X_train),hist=False,color=\"b\",label=\"Predicted Values\",ax=axes[0])\n",
    "        axes[0].set(title=\"Actual vs Predicted Train Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        sns.distplot(y_test, hist=False ,color=\"r\", label=\"Actual Value\",ax=axes[1])\n",
    "        sns.distplot(svr_model.predict(X_test),hist=False,color=\"b\",label=\"Predicted Value\",ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[1].legend(loc='best')   \n",
    "    \n",
    "    b=optimum_root_mean_square_error( svr_model, X_train, X_test, y_train, y_test)\n",
    "    c=optimum_R_squred_Score(svr_model ,X_train, y_train)\n",
    "    d=Distributions_and_Variance_of_Test_Residuals(svr_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    e=Harmonies_of_Actual_values_with_Predicted_values(svr_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    \n",
    "    return a,display(b),display(c), d, e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac200e5-1efe-49ba-ad01-01b6b4acc8ad",
   "metadata": {},
   "source": [
    "# 3-)Multiple Layers Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a520f4-69fb-4183-890e-e265590ee348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import kurtosis, skew\n",
    "from IPython.display import display\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "\n",
    "def Multiple_Layers_Regression_Model(independent_variables, target_variable):\n",
    "    print(\"*************************************************Multiple Layers Regression Model*********************************************************\")\n",
    "    \n",
    "    \n",
    "    def optimum_parameters(independent_variables, target_variable):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "        X_train_scaled=StandardScaler().fit_transform(X_train) \n",
    "        X_test_scaled=StandardScaler().fit_transform(X_test)\n",
    "        mlp_params = {'alpha':[ 0.1, 0.05, 0.025, 0.013 , 0.007, 0.001, 0.0005, 0.0001],\n",
    "             'hidden_layer_sizes': [(75,75), (50,100) ,(100,200) ,(100,50,150),(100,100,100)],\n",
    "             'activation': ['relu','logistic']}\n",
    "        mlp_model = MLPRegressor()\n",
    "        mlp_cv_model = GridSearchCV(mlp_model, mlp_params, cv = 10,n_jobs=-1)\n",
    "        mlp_cv_model.fit(X_train_scaled, y_train)\n",
    "        optimum_parameters=mlp_cv_model.best_params_\n",
    "        print(\"Optimum model parameters are {}\".format(optimum_parameters))\n",
    "        return optimum_parameters\n",
    "    \n",
    "    a=optimum_parameters(independent_variables, target_variable)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "    X_train_scaled=StandardScaler().fit_transform(X_train) \n",
    "    X_test_scaled=StandardScaler().fit_transform(X_test)\n",
    "    \n",
    "    mlp_model = MLPRegressor(alpha = a['alpha'], hidden_layer_sizes = a['hidden_layer_sizes'],activation=a['activation']).fit(X_train_scaled, y_train)\n",
    "    \n",
    "    \n",
    "    def optimum_root_mean_square_error( mlp_model, X_train_scaled, X_test_scaled, y_train, y_test):# Bu kismda  diger  modelerle karsilastirip en iyi modeli secmek icin  opitimum rmse kullanacagiz  \n",
    "        optimum_rmse_test=np.sqrt(-cross_val_score( mlp_model, X_test_scaled, y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean()#K KATLI cross validtion  uygulama yaptiktan sonnraki TEST hatamiz\n",
    "        optimum_rmse_train=np.sqrt(-cross_val_score( mlp_model, X_train_scaled, y_train, cv = 10, scoring = \"neg_mean_squared_error\")).mean()#K KATLI cross validtion  uygulama yaptiktan sonnraki TRAIN hatamiz\n",
    "        df1=pd.DataFrame(optimum_rmse_test, index=[\"optimum_RMSE_test\"],columns=['Coefficient'])\n",
    "        df2=pd.DataFrame(optimum_rmse_train, index=[\"optimum_RMSE_train\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    def optimum_R_squred_Score(mlp_model ,X_train_scaled, y_train):#R^2 (R-square degeridir).#Yani bagimsiz degiskenler  bagimli degiskendeki varyasin yuzde kacini aciklamislardir\n",
    "        R2_score=cross_val_score(mlp_model, X_train_scaled , y_train, cv = 10, scoring = \"r2\").mean()#burada egitim setimiz icerisinden her saferinde farkli fozlemler alindigindan dolayi \n",
    "        df1=pd.DataFrame(R2_score, index=[\"optimum_R2_SCORE\"],columns=['Coefficient'])                   #10 adet farkli  r2 score  degeri olustur duk ve bunlarin ortalam degerlerini aldik\n",
    "        return df1\n",
    "    \n",
    "    \n",
    "    \n",
    "    def Distributions_and_Variance_of_Test_Residuals(mlp_model,   X_test_scaled,   y_test,   X_train_scaled,   y_train):\n",
    "        predictions_test = mlp_model.predict(X_test_scaled)# Olusturdugumuz modelin test degerleri icin olusturdugu tahmini degerler\n",
    "        residual_test= y_test-predictions_test\n",
    "\n",
    "\n",
    "        predictions_train = mlp_model.predict(X_train_scaled)# Olusturdugumuz modelin train degerleri icin olusturdugu tahmini degerler\n",
    "        residual_train= y_train-predictions_train\n",
    "\n",
    "        fig, axes = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        df_test=pd.DataFrame({\"Actual_Dependent_Values\":y_test,\n",
    "                             \"Predicted_Dependent_Values\":predictions_test,\n",
    "                             \"Residuals\":residual_test}).reset_index(drop=True)\n",
    "        \n",
    "        df_train=pd.DataFrame({\"Actual_Dependent_Values\":y_train,\n",
    "                              \"Predicted_Dependent_Values\":predictions_train,\n",
    "                              \"Residuals\":residual_train}).reset_index(drop=True)\n",
    "              \n",
    "        fig.suptitle(\"Distributions and Variance of Residuals \")\n",
    "\n",
    "        a= \"Skewness: %.2f\" % residual_test.skew()\n",
    "        b=\"Kurtosis: %.2f\" % residual_test.kurtosis()\n",
    "        c=\"Mean: %.2f\" %   residual_test.mean()\n",
    "        d=\"Median: %.2f\" % residual_test.median()\n",
    "\n",
    "\n",
    "        sns.distplot(df_test[\"Residuals\"], bins=20,ax=axes[0])\n",
    "        axes[0].set(title=\"{} {} {} {}\".format(a,b,c,d), \n",
    "                    xlabel=\"Value of Deviations\", \n",
    "                    ylabel=\"Frequency\") \n",
    "     \n",
    "    \n",
    "\n",
    "        sns.scatterplot(x=\"Actual_Dependent_Values\", y=\"Predicted_Dependent_Values\",data=df_test, ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                    xlabel=\" Actual Test Values\", \n",
    "                    ylabel=\"Predicted Test Values\")\n",
    "            \n",
    "\n",
    "\n",
    "        sns.lineplot(x=df_test.index,y=df_test[\"Residuals\"],data=df_test,ax=axes[2])\n",
    "        axes[2].set(title=\"Residuals of Test Values  \", \n",
    "                    xlabel=\"Indexes of Test Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "\n",
    "        sns.lineplot(x=df_train.index,y=df_train[\"Residuals\"],data=df_train,ax=axes[3])\n",
    "        axes[3].set(title=\"Residuals of Train Values  \", \n",
    "                    xlabel=\"Indexes of Train Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "    def Harmonies_of_Actual_values_with_Predicted_values(mlp_model,   X_test_scaled,   y_test,   X_train_scaled,   y_train):\n",
    "        fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "        fig.suptitle(\"Distributions of Test & Train values and their harmonies with Predicted values\")\n",
    "        sns.distplot(y_train, hist=False ,color=\"r\", label=\"Actual Values\",ax=axes[0])\n",
    "        sns.distplot(mlp_model.predict(X_train_scaled),hist=False,color=\"b\",label=\"Predicted Values\",ax=axes[0])\n",
    "        axes[0].set(title=\"Actual vs Predicted Train Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        sns.distplot(y_test, hist=False ,color=\"r\", label=\"Actual Value\",ax=axes[1])\n",
    "        sns.distplot(mlp_model.predict(X_test_scaled),hist=False,color=\"b\",label=\"Predicted Value\",ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[1].legend(loc='best')\n",
    "    \n",
    "    b=optimum_root_mean_square_error( mlp_model, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "    c= optimum_R_squred_Score(mlp_model ,X_train_scaled, y_train)\n",
    "    d=Distributions_and_Variance_of_Test_Residuals(mlp_model,   X_test_scaled,   y_test,   X_train_scaled,   y_train)\n",
    "    e=Harmonies_of_Actual_values_with_Predicted_values(mlp_model,   X_test_scaled,   y_test,   X_train_scaled,   y_train)\n",
    "    \n",
    "    return a ,display(b),display(c),d ,e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5846fb9a-06e1-424d-b496-6cef551aafad",
   "metadata": {},
   "source": [
    "# 4-)CART ( Classification and Regressions Trees) Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01cbbd4-0d38-4f84-b81b-dee5382bb605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import kurtosis, skew\n",
    "from IPython.display import display\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "\n",
    "\n",
    "def CART_Regression_Model(independent_variables, target_variable):\n",
    "    print(\"*************************************************CART Regression Model*********************************************************\")\n",
    "    \n",
    "    def optimum_parameters(independent_variables, target_variable):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "        cart_params = {\"min_samples_split\": range(2,100),\n",
    "                       \"max_leaf_nodes\": range(2,10)}\n",
    "        cart_model = DecisionTreeRegressor()\n",
    "        cart_cv_model = GridSearchCV(cart_model, cart_params, cv = 10)\n",
    "        cart_cv_model.fit(X_train, y_train)\n",
    "        optimum_parameters=cart_cv_model.best_params_\n",
    "        print(\"Optimum model parameters are {}\".format(optimum_parameters))\n",
    "        return optimum_parameters\n",
    "    \n",
    "    a=optimum_parameters(independent_variables, target_variable)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "    cart_model = DecisionTreeRegressor(max_leaf_nodes=a['max_leaf_nodes'],min_samples_split = a['min_samples_split']).fit(X_train, y_train)\n",
    "    \n",
    "    def optimum_root_mean_square_error( cart_model, X_train, X_test, y_train, y_test):\n",
    "        optimum_rmse_test=np.sqrt(-cross_val_score( cart_model, X_test, y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        optimum_rmse_train=np.sqrt(-cross_val_score( cart_model, X_train, y_train, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        df1=pd.DataFrame(optimum_rmse_test, index=[\"optimum_RMSE_test\"],columns=['Coefficient'])\n",
    "        df2=pd.DataFrame(optimum_rmse_train, index=[\"optimum_RMSE_train\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    def optimum_R_squred_Score(cart_model ,X_train, y_train):\n",
    "        R2_score=cross_val_score(cart_model, X_train , y_train, cv = 10, scoring = \"r2\").mean()\n",
    "        df1=pd.DataFrame(R2_score, index=[\"optimum_R2_SCORE\"],columns=['Coefficient'])                  \n",
    "        return df1\n",
    "    \n",
    "    def Distributions_and_Variance_of_Test_Residuals(cart_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        predictions_test = cart_model.predict(X_test)\n",
    "        residual_test= y_test-predictions_test\n",
    "\n",
    "\n",
    "        predictions_train = cart_model.predict(X_train)\n",
    "        residual_train= y_train-predictions_train\n",
    "\n",
    "        fig, axes = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        df_test=pd.DataFrame({\"Actual_Dependent_Values\":y_test,\n",
    "                             \"Predicted_Dependent_Values\":predictions_test,\n",
    "                             \"Residuals\":residual_test}).reset_index(drop=True)\n",
    "        \n",
    "        df_train=pd.DataFrame({\"Actual_Dependent_Values\":y_train,\n",
    "                              \"Predicted_Dependent_Values\":predictions_train,\n",
    "                              \"Residuals\":residual_train}).reset_index(drop=True)\n",
    "              \n",
    "        fig.suptitle(\"Distributions and Variance of Residuals \")\n",
    "\n",
    "        a= \"Skewness: %.2f\" % residual_test.skew()\n",
    "        b=\"Kurtosis: %.2f\" % residual_test.kurtosis()\n",
    "        c=\"Mean: %.2f\" %   residual_test.mean()\n",
    "        d=\"Median: %.2f\" % residual_test.median()\n",
    "\n",
    "\n",
    "        sns.distplot(df_test[\"Residuals\"], bins=20,ax=axes[0])\n",
    "        axes[0].set(title=\"{} {} {} {}\".format(a,b,c,d), \n",
    "                    xlabel=\"Value of Deviations\", \n",
    "                    ylabel=\"Frequency\") \n",
    "     \n",
    "    \n",
    "\n",
    "        sns.scatterplot(x=\"Actual_Dependent_Values\", y=\"Predicted_Dependent_Values\",data=df_test, ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                    xlabel=\" Actual Test Values\", \n",
    "                    ylabel=\"Predicted Test Values\")\n",
    "            \n",
    "\n",
    "\n",
    "        sns.lineplot(x=df_test.index,y=df_test[\"Residuals\"],data=df_test,ax=axes[2])\n",
    "        axes[2].set(title=\"Residuals of Test Values  \", \n",
    "                    xlabel=\"Indexes of Test Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "\n",
    "        sns.lineplot(x=df_train.index,y=df_train[\"Residuals\"],data=df_train,ax=axes[3])\n",
    "        axes[3].set(title=\"Residuals of Train Values  \", \n",
    "                    xlabel=\"Indexes of Train Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "    def Harmonies_of_Actual_values_with_Predicted_values(cart_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "        fig.suptitle(\"Distributions of Test & Train values and their harmonies with Predicted values\")\n",
    "        sns.distplot(y_train, hist=False ,color=\"r\", label=\"Actual Values\",ax=axes[0])\n",
    "        sns.distplot(cart_model.predict(X_train),hist=False,color=\"b\",label=\"Predicted Values\",ax=axes[0])\n",
    "        axes[0].set(title=\"Actual vs Predicted Train Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        sns.distplot(y_test, hist=False ,color=\"r\", label=\"Actual Value\",ax=axes[1])\n",
    "        sns.distplot(cart_model.predict(X_test),hist=False,color=\"b\",label=\"Predicted Value\",ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[1].legend(loc='best') \n",
    "                          \n",
    "                                                        \n",
    "                                                        \n",
    "    b=optimum_root_mean_square_error( cart_model, X_train, X_test, y_train, y_test)\n",
    "    c=optimum_R_squred_Score(cart_model ,X_train, y_train)\n",
    "    d=Distributions_and_Variance_of_Test_Residuals(cart_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    e=Harmonies_of_Actual_values_with_Predicted_values(cart_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    \n",
    "    return a, display(b) ,display(c),d,e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a83b370-a316-4dd2-ae6c-21fb9ad48f5e",
   "metadata": {},
   "source": [
    "# 5-) Bagged Trees Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea95baf-fbe4-459b-8190-dc304c300d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import kurtosis, skew\n",
    "from IPython.display import display\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Bagged_Trees_Regression_Model(independent_variables, target_variable):\n",
    "    print(\"*************************************************Bagged Trees  Regression  Model*********************************************************\")\n",
    "    \n",
    "    def optimum_number_of_trees(independent_variables, target_variable):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "        bag_params = {\"n_estimators\": range(2,30)}\n",
    "        bag_model=BaggingRegressor(bootstrap_features = True)\n",
    "        bag_cv_model = GridSearchCV(bag_model, bag_params, cv = 10)\n",
    "        bag_cv_model.fit(X_train, y_train)\n",
    "        optimum_number_of_trees=bag_cv_model.best_params_\n",
    "        print(\"Optimum numberof trees  is {}\".format( optimum_number_of_trees))\n",
    "        return optimum_number_of_trees\n",
    "    \n",
    "    a=optimum_number_of_trees(independent_variables, target_variable)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "    bag_model= BaggingRegressor(bootstrap_features = True, n_estimators =a['n_estimators'],random_state=101).fit(X_train,y_train)\n",
    "    \n",
    "    \n",
    "    def optimum_root_mean_square_error(  bag_model, X_train, X_test, y_train, y_test):\n",
    "        optimum_rmse_test=np.sqrt(-cross_val_score( bag_model, X_test, y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        optimum_rmse_train=np.sqrt(-cross_val_score( bag_model, X_train, y_train, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        df1=pd.DataFrame(optimum_rmse_test, index=[\"optimum_RMSE_test\"],columns=['Coefficient'])\n",
    "        df2=pd.DataFrame(optimum_rmse_train, index=[\"optimum_RMSE_train\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    def optimum_R_squred_Score(bag_model ,X_train, y_train):\n",
    "        R2_score=cross_val_score(bag_model, X_train , y_train, cv = 10, scoring = \"r2\").mean()\n",
    "        df1=pd.DataFrame(R2_score, index=[\"optimum_R2_SCORE\"],columns=['Coefficient'])                  \n",
    "        return df1\n",
    "    \n",
    "    def Distributions_and_Variance_of_Test_Residuals(bag_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        predictions_test = bag_model.predict(X_test)\n",
    "        residual_test= y_test-predictions_test\n",
    "\n",
    "\n",
    "        predictions_train = bag_model.predict(X_train)\n",
    "        residual_train= y_train-predictions_train\n",
    "\n",
    "        fig, axes = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        df_test=pd.DataFrame({\"Actual_Dependent_Values\":y_test,\n",
    "                             \"Predicted_Dependent_Values\":predictions_test,\n",
    "                             \"Residuals\":residual_test}).reset_index(drop=True)\n",
    "        \n",
    "        df_train=pd.DataFrame({\"Actual_Dependent_Values\":y_train,\n",
    "                              \"Predicted_Dependent_Values\":predictions_train,\n",
    "                              \"Residuals\":residual_train}).reset_index(drop=True)\n",
    "              \n",
    "        fig.suptitle(\"Distributions and Variance of Residuals \")\n",
    "\n",
    "        a= \"Skewness: %.2f\" % residual_test.skew()\n",
    "        b=\"Kurtosis: %.2f\" % residual_test.kurtosis()\n",
    "        c=\"Mean: %.2f\" %   residual_test.mean()\n",
    "        d=\"Median: %.2f\" % residual_test.median()\n",
    "\n",
    "\n",
    "        sns.distplot(df_test[\"Residuals\"], bins=20,ax=axes[0])\n",
    "        axes[0].set(title=\"{} {} {} {}\".format(a,b,c,d), \n",
    "                    xlabel=\"Value of Deviations\", \n",
    "                    ylabel=\"Frequency\") \n",
    "     \n",
    "    \n",
    "\n",
    "        sns.scatterplot(x=\"Actual_Dependent_Values\", y=\"Predicted_Dependent_Values\",data=df_test, ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                    xlabel=\" Actual Test Values\", \n",
    "                    ylabel=\"Predicted Test Values\")\n",
    "            \n",
    "\n",
    "\n",
    "        sns.lineplot(x=df_test.index,y=df_test[\"Residuals\"],data=df_test,ax=axes[2])\n",
    "        axes[2].set(title=\"Residuals of Test Values  \", \n",
    "                    xlabel=\"Indexes of Test Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "\n",
    "        sns.lineplot(x=df_train.index,y=df_train[\"Residuals\"],data=df_train,ax=axes[3])\n",
    "        axes[3].set(title=\"Residuals of Train Values  \", \n",
    "                    xlabel=\"Indexes of Train Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "        \n",
    "    def Harmonies_of_Actual_values_with_Predicted_values(bag_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "        fig.suptitle(\"Distributions of Test & Train values and their harmonies with Predicted values\")\n",
    "        sns.distplot(y_train, hist=False ,color=\"r\", label=\"Actual Values\",ax=axes[0])\n",
    "        sns.distplot(bag_model.predict(X_train),hist=False,color=\"b\",label=\"Predicted Values\",ax=axes[0])\n",
    "        axes[0].set(title=\"Actual vs Predicted Train Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        sns.distplot(y_test, hist=False ,color=\"r\", label=\"Actual Value\",ax=axes[1])\n",
    "        sns.distplot(bag_model.predict(X_test),hist=False,color=\"b\",label=\"Predicted Value\",ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[1].legend(loc='best')    \n",
    "    \n",
    "    \n",
    "    \n",
    "    b=optimum_root_mean_square_error(  bag_model, X_train, X_test, y_train, y_test)\n",
    "    c=optimum_R_squred_Score(bag_model ,X_train, y_train)\n",
    "    d=Distributions_and_Variance_of_Test_Residuals(bag_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    e=Harmonies_of_Actual_values_with_Predicted_values(bag_model,   X_test,   y_test,   X_train,   y_train)\n",
    "     \n",
    "    return a ,display(b),display(c),d,e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e11db-1874-4f43-92b6-f020f4b3ee7f",
   "metadata": {},
   "source": [
    "# 6-)Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3157ae-590d-4e91-9491-0f0af781c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import kurtosis, skew\n",
    "from IPython.display import display\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "\n",
    "def Random_Forest_Regression_Model(independent_variables, target_variable):\n",
    "    print(\"*************************************************Random Forest Regression  Model*********************************************************\")\n",
    "    \n",
    "    def optimum_parameters(independent_variables, target_variable):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "        rf_params = {'max_depth': list(range(1,20)),\n",
    "                     'max_features': [3,5,10,15,25,100],\n",
    "                     'n_estimators' : [10,50,100, 200, 500, 1000, 2000]}\n",
    "        rf_model = RandomForestRegressor(random_state = 101)\n",
    "        rf_cv_model = GridSearchCV(rf_model, rf_params, cv = 10, n_jobs = -1)\n",
    "        rf_cv_model.fit(X_train, y_train)\n",
    "        optimum_parameters=rf_cv_model.best_params_\n",
    "        print(\"Optimum model parameters are {}\".format(optimum_parameters))\n",
    "        return optimum_parameters\n",
    "    \n",
    "    a=optimum_parameters(independent_variables, target_variable)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "    rf_model= RandomForestRegressor(random_state = 101,max_depth=a['max_depth'],max_features=a['max_features'],n_estimators=a['n_estimators']).fit(X_train, y_train)\n",
    "    \n",
    "    def optimum_root_mean_square_error( rf_model, X_train, X_test, y_train, y_test):\n",
    "        optimum_rmse_test=np.sqrt(-cross_val_score( rf_model, X_test, y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        optimum_rmse_train=np.sqrt(-cross_val_score( rf_model, X_train, y_train, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        df1=pd.DataFrame(optimum_rmse_test, index=[\"optimum_RMSE_test\"],columns=['Coefficient'])\n",
    "        df2=pd.DataFrame(optimum_rmse_train, index=[\"optimum_RMSE_train\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    def optimum_R_squred_Score(rf_model,X_train, y_train):\n",
    "        R2_score=cross_val_score(rf_model, X_train , y_train, cv = 10, scoring = \"r2\").mean()\n",
    "        df1=pd.DataFrame(R2_score, index=[\"optimum_R2_SCORE\"],columns=['Coefficient'])                  \n",
    "        return df1\n",
    "    \n",
    "    def Distributions_and_Variance_of_Test_Residuals(rf_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        predictions_test = rf_model.predict(X_test)\n",
    "        residual_test= y_test-predictions_test\n",
    "\n",
    "\n",
    "        predictions_train = rf_model.predict(X_train)\n",
    "        residual_train= y_train-predictions_train\n",
    "\n",
    "        fig, axes = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        df_test=pd.DataFrame({\"Actual_Dependent_Values\":y_test,\n",
    "                             \"Predicted_Dependent_Values\":predictions_test,\n",
    "                             \"Residuals\":residual_test}).reset_index(drop=True)\n",
    "        \n",
    "        df_train=pd.DataFrame({\"Actual_Dependent_Values\":y_train,\n",
    "                              \"Predicted_Dependent_Values\":predictions_train,\n",
    "                              \"Residuals\":residual_train}).reset_index(drop=True)\n",
    "              \n",
    "        fig.suptitle(\"Distributions and Variance of Residuals \")\n",
    "\n",
    "        a= \"Skewness: %.2f\" % residual_test.skew()\n",
    "        b=\"Kurtosis: %.2f\" % residual_test.kurtosis()\n",
    "        c=\"Mean: %.2f\" %   residual_test.mean()\n",
    "        d=\"Median: %.2f\" % residual_test.median()\n",
    "\n",
    "\n",
    "        sns.distplot(df_test[\"Residuals\"], bins=20,ax=axes[0])\n",
    "        axes[0].set(title=\"{} {} {} {}\".format(a,b,c,d), \n",
    "                    xlabel=\"Value of Deviations\", \n",
    "                    ylabel=\"Frequency\") \n",
    "     \n",
    "    \n",
    "\n",
    "        sns.scatterplot(x=\"Actual_Dependent_Values\", y=\"Predicted_Dependent_Values\",data=df_test, ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                    xlabel=\" Actual Test Values\", \n",
    "                    ylabel=\"Predicted Test Values\")\n",
    "            \n",
    "\n",
    "\n",
    "        sns.lineplot(x=df_test.index,y=df_test[\"Residuals\"],data=df_test,ax=axes[2])\n",
    "        axes[2].set(title=\"Residuals of Test Values  \", \n",
    "                    xlabel=\"Indexes of Test Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "\n",
    "        sns.lineplot(x=df_train.index,y=df_train[\"Residuals\"],data=df_train,ax=axes[3])\n",
    "        axes[3].set(title=\"Residuals of Train Values  \", \n",
    "                    xlabel=\"Indexes of Train Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "    \n",
    "    def Harmonies_of_Actual_values_with_Predicted_values(rf_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "        fig.suptitle(\"Distributions of Test & Train values and their harmonies with Predicted values\")\n",
    "        sns.distplot(y_train, hist=False ,color=\"r\", label=\"Actual Values\",ax=axes[0])\n",
    "        sns.distplot(rf_model.predict(X_train),hist=False,color=\"b\",label=\"Predicted Values\",ax=axes[0])\n",
    "        axes[0].set(title=\"Actual vs Predicted Train Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        sns.distplot(y_test, hist=False ,color=\"r\", label=\"Actual Value\",ax=axes[1])\n",
    "        sns.distplot(rf_model.predict(X_test),hist=False,color=\"b\",label=\"Predicted Value\",ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[1].legend(loc='best')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    b=optimum_root_mean_square_error( rf_model, X_train, X_test, y_train, y_test)\n",
    "    c=optimum_R_squred_Score(rf_model,X_train, y_train)\n",
    "    d=Distributions_and_Variance_of_Test_Residuals(rf_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    e=Harmonies_of_Actual_values_with_Predicted_values(rf_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    \n",
    "    return a,display(b),display(c),d,e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c5f6c-a6f5-4d2b-8514-5b5d8dbe37d9",
   "metadata": {},
   "source": [
    "# 7-)Gradient Boosting Machines (GBM)  Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f6559-3977-46f1-9e10-e12062fb3746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from scipy.stats import kurtosis, skew\n",
    "from IPython.display import display\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "\n",
    "\n",
    "\n",
    "def Gradient_Boosting_Machine_Regression_Model(independent_variables, target_variable):\n",
    "    print(\"*************************************************Gradient Boosting Machine RegressionModel*********************************************************\")\n",
    "    \n",
    "    def optimum_parameters(independent_variables, target_variable):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "        gbm_params = {'learning_rate': [0.0001,0.001, 0.01, 0.1, 0.2, 0.5, 1, 10],\n",
    "                      'max_depth': [3, 5, 8,50,100,500],\n",
    "                      'n_estimators': [200, 500, 1000, 2000],\n",
    "                      'subsample': [1,0.5,0.75]}\n",
    "        \n",
    "        gbm = GradientBoostingRegressor()\n",
    "        gbm_cv_model = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1)\n",
    "        gbm_cv_model.fit(X_train, y_train)\n",
    "        optimum_parameters=gbm_cv_model.best_params_\n",
    "        print(\"Optimum model parameters are {}\".format(optimum_parameters))\n",
    "        return optimum_parameters\n",
    "    \n",
    "    a=optimum_parameters(independent_variables, target_variable)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "    gbm_model=GradientBoostingRegressor(learning_rate=a['learning_rate'],max_depth=a['max_depth'],n_estimators=a['n_estimators'],subsample=a['subsample']).fit(X_train,y_train)\n",
    "    \n",
    "    def optimum_root_mean_square_error( gbm_model, X_train, X_test, y_train, y_test):\n",
    "        optimum_rmse_test=np.sqrt(-cross_val_score( gbm_model, X_test, y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        optimum_rmse_train=np.sqrt(-cross_val_score( gbm_model, X_train, y_train, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        df1=pd.DataFrame(optimum_rmse_test, index=[\"optimum_RMSE_test\"],columns=['Coefficient'])\n",
    "        df2=pd.DataFrame(optimum_rmse_train, index=[\"optimum_RMSE_train\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    def optimum_R_squred_Score(gbm_model,X_train, y_train):\n",
    "        R2_score=cross_val_score(gbm_model, X_train , y_train, cv = 10, scoring = \"r2\").mean()\n",
    "        df1=pd.DataFrame(R2_score, index=[\"optimum_R2_SCORE\"],columns=['Coefficient'])                  \n",
    "        return df1\n",
    "    \n",
    "    def Distributions_and_Variance_of_Test_Residuals(gbm_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        predictions_test = gbm_model.predict(X_test)\n",
    "        residual_test= y_test-predictions_test\n",
    "\n",
    "\n",
    "        predictions_train = gbm_model.predict(X_train)\n",
    "        residual_train= y_train-predictions_train\n",
    "\n",
    "        fig, axes = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        df_test=pd.DataFrame({\"Actual_Dependent_Values\":y_test,\n",
    "                             \"Predicted_Dependent_Values\":predictions_test,\n",
    "                             \"Residuals\":residual_test}).reset_index(drop=True)\n",
    "        \n",
    "        df_train=pd.DataFrame({\"Actual_Dependent_Values\":y_train,\n",
    "                              \"Predicted_Dependent_Values\":predictions_train,\n",
    "                              \"Residuals\":residual_train}).reset_index(drop=True)\n",
    "              \n",
    "        fig.suptitle(\"Distributions and Variance of Residuals \")\n",
    "\n",
    "        a= \"Skewness: %.2f\" % residual_test.skew()\n",
    "        b=\"Kurtosis: %.2f\" % residual_test.kurtosis()\n",
    "        c=\"Mean: %.2f\" %   residual_test.mean()\n",
    "        d=\"Median: %.2f\" % residual_test.median()\n",
    "\n",
    "\n",
    "        sns.distplot(df_test[\"Residuals\"], bins=20,ax=axes[0])\n",
    "        axes[0].set(title=\"{} {} {} {}\".format(a,b,c,d), \n",
    "                    xlabel=\"Value of Deviations\", \n",
    "                    ylabel=\"Frequency\") \n",
    "     \n",
    "    \n",
    "\n",
    "        sns.scatterplot(x=\"Actual_Dependent_Values\", y=\"Predicted_Dependent_Values\",data=df_test, ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                    xlabel=\" Actual Test Values\", \n",
    "                    ylabel=\"Predicted Test Values\")\n",
    "            \n",
    "\n",
    "\n",
    "        sns.lineplot(x=df_test.index,y=df_test[\"Residuals\"],data=df_test,ax=axes[2])\n",
    "        axes[2].set(title=\"Residuals of Test Values  \", \n",
    "                    xlabel=\"Indexes of Test Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "\n",
    "        sns.lineplot(x=df_train.index,y=df_train[\"Residuals\"],data=df_train,ax=axes[3])\n",
    "        axes[3].set(title=\"Residuals of Train Values  \", \n",
    "                    xlabel=\"Indexes of Train Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "    def Harmonies_of_Actual_values_with_Predicted_values(gbm_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "        fig.suptitle(\"Distributions of Test & Train values and their harmonies with Predicted values\")\n",
    "        sns.distplot(y_train, hist=False ,color=\"r\", label=\"Actual Values\",ax=axes[0])\n",
    "        sns.distplot(gbm_model.predict(X_train),hist=False,color=\"b\",label=\"Predicted Values\",ax=axes[0])\n",
    "        axes[0].set(title=\"Actual vs Predicted Train Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        sns.distplot(y_test, hist=False ,color=\"r\", label=\"Actual Value\",ax=axes[1])\n",
    "        sns.distplot(gbm_model.predict(X_test),hist=False,color=\"b\",label=\"Predicted Value\",ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[1].legend(loc='best')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    b=optimum_root_mean_square_error( gbm_model, X_train, X_test, y_train, y_test)\n",
    "    c=optimum_R_squred_Score(gbm_model,X_train, y_train)\n",
    "    d= Distributions_and_Variance_of_Test_Residuals(gbm_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    e=Harmonies_of_Actual_values_with_Predicted_values(gbm_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    \n",
    "    \n",
    "    return a, display(b),display(c),d,e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92062442-1c37-4dfa-945e-d75a3078de2d",
   "metadata": {},
   "source": [
    "# 8-)Extreme Gradient Boosting (XGBoost) Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b101366c-74f6-43e9-8cdc-ad36cde56924",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import kurtosis, skew\n",
    "from IPython.display import display\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "\n",
    "\n",
    "\n",
    "def Extreme_Gradient_Boosting_Regression_Model(independent_variables, target_variable):\n",
    "    print(\"************************************************* Extreme Gradient Boosting  Regression Model******************************************************\")\n",
    "    \n",
    "    def optimum_parameters(independent_variables, target_variable):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "        xgb_grid = {'colsample_bytree': [0.2,0.4,0.6,0.9, 1, 1.2,1.6,2], \n",
    "                    'n_estimators':[100, 200, 500, 1000, 5000],\n",
    "                    'max_depth': [2,3,4,5,6,7,8,9],\n",
    "                    'learning_rate': [0.1, 0.01, 0.5,0.001]}\n",
    "        xgb = XGBRegressor()\n",
    "        xgb_cv = GridSearchCV(xgb, param_grid = xgb_grid, cv = 10, n_jobs = -1)\n",
    "        xgb_cv.fit(X_train, y_train)\n",
    "        optimum_parameters=xgb_cv.best_params_\n",
    "        print(\"Optimum model parameters are {}\".format(optimum_parameters))\n",
    "        return optimum_parameters\n",
    "    \n",
    "    a=optimum_parameters(independent_variables, target_variable)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "    xgb_model=XGBRegressor(colsample_bytree=a['colsample_bytree'],max_depth=a['max_depth'],n_estimators=a['n_estimators'],learning_rate=a['learning_rate']).fit(X_train,y_train)\n",
    "    \n",
    "    def optimum_root_mean_square_error( xgb_model, X_train, X_test, y_train, y_test):\n",
    "        optimum_rmse_test=np.sqrt(-cross_val_score( xgb_model, X_test, y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        optimum_rmse_train=np.sqrt(-cross_val_score( xgb_model, X_train, y_train, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        df1=pd.DataFrame(optimum_rmse_test, index=[\"optimum_RMSE_test\"],columns=['Coefficient'])\n",
    "        df2=pd.DataFrame(optimum_rmse_train, index=[\"optimum_RMSE_train\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    def optimum_R_squred_Score(xgb_model,X_train, y_train):\n",
    "        R2_score=cross_val_score(xgb_model, X_train , y_train, cv = 10, scoring = \"r2\").mean()\n",
    "        df1=pd.DataFrame(R2_score, index=[\"optimum_R2_SCORE\"],columns=['Coefficient'])                  \n",
    "        return df1\n",
    "    \n",
    "    \n",
    "    \n",
    "    def Distributions_and_Variance_of_Test_Residuals(xgb_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        predictions_test = xgb_model.predict(X_test)\n",
    "        residual_test= y_test-predictions_test\n",
    "\n",
    "\n",
    "        predictions_train = xgb_model.predict(X_train)\n",
    "        residual_train= y_train-predictions_train\n",
    "\n",
    "        fig, axes = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        df_test=pd.DataFrame({\"Actual_Dependent_Values\":y_test,\n",
    "                             \"Predicted_Dependent_Values\":predictions_test,\n",
    "                             \"Residuals\":residual_test}).reset_index(drop=True)\n",
    "        \n",
    "        df_train=pd.DataFrame({\"Actual_Dependent_Values\":y_train,\n",
    "                              \"Predicted_Dependent_Values\":predictions_train,\n",
    "                              \"Residuals\":residual_train}).reset_index(drop=True)\n",
    "              \n",
    "        fig.suptitle(\"Distributions and Variance of Residuals \")\n",
    "\n",
    "        a= \"Skewness: %.2f\" % residual_test.skew()\n",
    "        b=\"Kurtosis: %.2f\" % residual_test.kurtosis()\n",
    "        c=\"Mean: %.2f\" %   residual_test.mean()\n",
    "        d=\"Median: %.2f\" % residual_test.median()\n",
    "\n",
    "\n",
    "        sns.distplot(df_test[\"Residuals\"], bins=20,ax=axes[0])\n",
    "        axes[0].set(title=\"{} {} {} {}\".format(a,b,c,d), \n",
    "                    xlabel=\"Value of Deviations\", \n",
    "                    ylabel=\"Frequency\") \n",
    "     \n",
    "    \n",
    "\n",
    "        sns.scatterplot(x=\"Actual_Dependent_Values\", y=\"Predicted_Dependent_Values\",data=df_test, ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                    xlabel=\" Actual Test Values\", \n",
    "                    ylabel=\"Predicted Test Values\")\n",
    "            \n",
    "\n",
    "\n",
    "        sns.lineplot(x=df_test.index,y=df_test[\"Residuals\"],data=df_test,ax=axes[2])\n",
    "        axes[2].set(title=\"Residuals of Test Values  \", \n",
    "                    xlabel=\"Indexes of Test Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "\n",
    "        sns.lineplot(x=df_train.index,y=df_train[\"Residuals\"],data=df_train,ax=axes[3])\n",
    "        axes[3].set(title=\"Residuals of Train Values  \", \n",
    "                    xlabel=\"Indexes of Train Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "        \n",
    "    def Harmonies_of_Actual_values_with_Predicted_values(xgb_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "        fig.suptitle(\"Distributions of Test & Train values and their harmonies with Predicted values\")\n",
    "        sns.distplot(y_train, hist=False ,color=\"r\", label=\"Actual Values\",ax=axes[0])\n",
    "        sns.distplot(xgb_model.predict(X_train),hist=False,color=\"b\",label=\"Predicted Values\",ax=axes[0])\n",
    "        axes[0].set(title=\"Actual vs Predicted Train Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        sns.distplot(y_test, hist=False ,color=\"r\", label=\"Actual Value\",ax=axes[1])\n",
    "        sns.distplot(xgb_model.predict(X_test),hist=False,color=\"b\",label=\"Predicted Value\",ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[1].legend(loc='best')\n",
    "    \n",
    "    \n",
    "    b=optimum_root_mean_square_error( xgb_model, X_train, X_test, y_train, y_test)\n",
    "    c=optimum_R_squred_Score(xgb_model,X_train, y_train)\n",
    "    d=Distributions_and_Variance_of_Test_Residuals(xgb_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    e=Harmonies_of_Actual_values_with_Predicted_values(xgb_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    \n",
    "    return  a,display(b),display(c),d,e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3fe0a8-2c37-44b0-bef7-39eeb4bc0753",
   "metadata": {},
   "source": [
    "# 9-)Light GBM  Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251da85c-ae9f-403d-9ff9-3202b2b1772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "from scipy.stats import kurtosis, skew\n",
    "from IPython.display import display\n",
    "plt.style.use(\"seaborn-darkgrid\")\n",
    "\n",
    "\n",
    "def Light_GBM_Regression_Model(independent_variables, target_variable):\n",
    "    print(\"************************************************* Light GBM  Regression Model*********************************************************\")\n",
    "    \n",
    "    def optimum_parameters(independent_variables, target_variable):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "        lgbm_grid = {'colsample_bytree': [0.2,0.4,0.6,0.9, 1, 1.2,1.6,2],\n",
    "                     'learning_rate': [0.1, 0.01, 0.5,0.001],\n",
    "                     'n_estimators': [100, 200, 500, 1000, 5000],\n",
    "                     'max_depth': [2,3,4,5,6,7,8,9] }\n",
    "        lgbm = LGBMRegressor()\n",
    "        lgbm_cv_model = GridSearchCV(lgbm, param_grid = lgbm_grid, cv=10, n_jobs = -1)\n",
    "        lgbm_cv_model.fit(X_train,y_train)\n",
    "        optimum_parameters=lgbm_cv_model.best_params_\n",
    "        print(\"Optimum model parameters are {}\".format(optimum_parameters))\n",
    "        return optimum_parameters\n",
    "    \n",
    "    a=optimum_parameters(independent_variables, target_variable)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(independent_variables, target_variable, test_size = 0.25, random_state= 101)\n",
    "    lgbm_model=LGBMRegressor(colsample_bytree=a['colsample_bytree'],max_depth=a['max_depth'],n_estimators=a['n_estimators'],learning_rate=a['learning_rate']).fit(X_train,y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def optimum_root_mean_square_error( lgbm_model, X_train, X_test, y_train, y_test):\n",
    "        optimum_rmse_test=np.sqrt(-cross_val_score( lgbm_model, X_test, y_test, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        optimum_rmse_train=np.sqrt(-cross_val_score( lgbm_model, X_train, y_train, cv = 10, scoring = \"neg_mean_squared_error\")).mean()\n",
    "        df1=pd.DataFrame(optimum_rmse_test, index=[\"optimum_RMSE_test\"],columns=['Coefficient'])\n",
    "        df2=pd.DataFrame(optimum_rmse_train, index=[\"optimum_RMSE_train\"],columns=['Coefficient'])\n",
    "        df_concat=pd.concat([df2,df1],axis=0)\n",
    "        return df_concat\n",
    "    \n",
    "    def optimum_R_squred_Score(lgbm_model,X_train, y_train):\n",
    "        R2_score=cross_val_score(lgbm_model, X_train , y_train, cv = 10, scoring = \"r2\").mean()\n",
    "        df1=pd.DataFrame(R2_score, index=[\"optimum_R2_SCORE\"],columns=['Coefficient'])                  \n",
    "        return df1\n",
    "    \n",
    "    \n",
    "    def Distributions_and_Variance_of_Test_Residuals(lgbm_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        predictions_test = lgbm_model.predict(X_test)\n",
    "        residual_test= y_test-predictions_test\n",
    "\n",
    "\n",
    "        predictions_train = lgbm_model.predict(X_train)\n",
    "        residual_train= y_train-predictions_train\n",
    "\n",
    "        fig, axes = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "        plt.style.use(\"seaborn-darkgrid\")\n",
    "        df_test=pd.DataFrame({\"Actual_Dependent_Values\":y_test,\n",
    "                             \"Predicted_Dependent_Values\":predictions_test,\n",
    "                             \"Residuals\":residual_test}).reset_index(drop=True)\n",
    "        \n",
    "        df_train=pd.DataFrame({\"Actual_Dependent_Values\":y_train,\n",
    "                              \"Predicted_Dependent_Values\":predictions_train,\n",
    "                              \"Residuals\":residual_train}).reset_index(drop=True)\n",
    "              \n",
    "        fig.suptitle(\"Distributions and Variance of Residuals \")\n",
    "\n",
    "        a= \"Skewness: %.2f\" % residual_test.skew()\n",
    "        b=\"Kurtosis: %.2f\" % residual_test.kurtosis()\n",
    "        c=\"Mean: %.2f\" %   residual_test.mean()\n",
    "        d=\"Median: %.2f\" % residual_test.median()\n",
    "\n",
    "\n",
    "        sns.distplot(df_test[\"Residuals\"], bins=20,ax=axes[0])\n",
    "        axes[0].set(title=\"{} {} {} {}\".format(a,b,c,d), \n",
    "                    xlabel=\"Value of Deviations\", \n",
    "                    ylabel=\"Frequency\") \n",
    "     \n",
    "    \n",
    "\n",
    "        sns.scatterplot(x=\"Actual_Dependent_Values\", y=\"Predicted_Dependent_Values\",data=df_test, ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                    xlabel=\" Actual Test Values\", \n",
    "                    ylabel=\"Predicted Test Values\")\n",
    "            \n",
    "\n",
    "\n",
    "        sns.lineplot(x=df_test.index,y=df_test[\"Residuals\"],data=df_test,ax=axes[2])\n",
    "        axes[2].set(title=\"Residuals of Test Values  \", \n",
    "                    xlabel=\"Indexes of Test Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "\n",
    "        sns.lineplot(x=df_train.index,y=df_train[\"Residuals\"],data=df_train,ax=axes[3])\n",
    "        axes[3].set(title=\"Residuals of Train Values  \", \n",
    "                    xlabel=\"Indexes of Train Values \", \n",
    "                    ylabel=\"Value of Deviations\")\n",
    "        \n",
    "        \n",
    "    def Harmonies_of_Actual_values_with_Predicted_values(lgbm_model,   X_test,   y_test,   X_train,   y_train):\n",
    "        fig, axes = plt.subplots(1,2, figsize=(15,7))\n",
    "        fig.suptitle(\"Distributions of Test & Train values and their harmonies with Predicted values\")\n",
    "        sns.distplot(y_train, hist=False ,color=\"r\", label=\"Actual Values\",ax=axes[0])\n",
    "        sns.distplot(lgbm_model.predict(X_train),hist=False,color=\"b\",label=\"Predicted Values\",ax=axes[0])\n",
    "        axes[0].set(title=\"Actual vs Predicted Train Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[0].legend(loc='best')\n",
    "\n",
    "        sns.distplot(y_test, hist=False ,color=\"r\", label=\"Actual Value\",ax=axes[1])\n",
    "        sns.distplot(lgbm_model.predict(X_test),hist=False,color=\"b\",label=\"Predicted Value\",ax=axes[1])\n",
    "        axes[1].set(title=\"Actual vs Predicted Test Values\", \n",
    "                xlabel=y_train.to_frame().columns[0], \n",
    "                ylabel=\"Proportion\")\n",
    "        axes[1].legend(loc='best')\n",
    "    \n",
    "    \n",
    "    \n",
    "    b=optimum_root_mean_square_error( lgbm_model, X_train, X_test, y_train, y_test)\n",
    "    c=optimum_R_squred_Score(lgbm_model,X_train, y_train)\n",
    "    d=Distributions_and_Variance_of_Test_Residuals(lgbm_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    e=Harmonies_of_Actual_values_with_Predicted_values(lgbm_model,   X_test,   y_test,   X_train,   y_train)\n",
    "    \n",
    "    return a ,display(b), display(c),d,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde8af43-9765-4039-a9ad-43c3b2c92c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1dea8d9-1975-4234-bc1d-38ecb33ed454",
   "metadata": {},
   "source": [
    "# Codes are running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39265023-0fe3-4014-a822-0e20a7b42157",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_Nearest_Neighbors_Regression_Model(independent_variables, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b48052-9b60-4070-ab08-e43a434ede04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Support_Vector_Regression_Model(independent_variables, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87505e91-d29d-4544-aa9f-b155a194a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiple_Layers_Regression_Model(independent_variables, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e586336-b448-4282-86b4-a837399ed3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART_Regression_Model(independent_variables, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d1270-2847-4d44-af17-e930b2911e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bagged_Trees_Regression_Model(independent_variables, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4602c-da8e-4ac6-b9b7-2751f064f0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random_Forest_Regression_Model(independent_variables, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df5f33f-7a4a-466f-9478-be524a5ec827",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient_Boosting_Machine_Regression_Model(independent_variables, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0948fd64-a951-416a-a9a9-5d0cc009e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "Extreme_Gradient_Boosting_Regression_Model(independent_variables, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a735840-502c-4966-bbcb-3375ccf58b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Light_GBM_Regression_Model(independent_variables, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776c0b93-b08d-4fcf-ac7e-253abf182140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
